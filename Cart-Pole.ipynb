{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.0.0\n",
      "Keras 2.3.1\n",
      "gym 0.10.5\n",
      "plotly 4.1.1\n",
      "pandas 0.25.2\n",
      "numpy 1.17.2\n"
     ]
    }
   ],
   "source": [
    "# based on Deep Learning Illustrated by Jon Krohn\n",
    "# https://www.amazon.com/Deep-Learning-Illustrated-Intelligence-Addison-Wesley/dp/0135116694\n",
    "# in turn based on bit.ly/keonDQN\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import resource\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Model, Sequential, load_model\n",
    "# from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import tensorflow.keras.backend as K\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# requires python 3.6\n",
    "# conda install -c akode gym\n",
    "import gym\n",
    "\n",
    "# set seeds for reproducibility\n",
    "# np.random.uniform(0,10000) 4465\n",
    "GLOBAL_SEED = 4465\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "\n",
    "print(\"TensorFlow %s\" % tf.__version__)\n",
    "print(\"Keras %s\" % keras.__version__)\n",
    "print(\"gym %s\" % gym.__version__)\n",
    "print(\"plotly %s\" % plotly.__version__)\n",
    "print(\"pandas %s\" % pd.__version__)\n",
    "print(\"numpy %s\" % np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.98\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "        self.memory = pd.DataFrame(columns=[\"state\", \"action\", \"next_state\", \"reward\", \"done\"])\n",
    "        self.memory_size=200000\n",
    "        self.results = []\n",
    "        self.train_batch_size=8\n",
    "        self.timestep=0\n",
    "        self.save_interval=10\n",
    "        \n",
    "    def build_model(self,\n",
    "                    n_hidden_layers=2, \n",
    "                    hidden_layer_size=16, \n",
    "                    activation='relu',\n",
    "                    reg_penalty=0.001,\n",
    "                    dropout=0.0675,\n",
    "                    verbose=True\n",
    "                   ):\n",
    "        \"\"\"return keras NN model per inputs\n",
    "        input is a state - array of size state_size\n",
    "        output is an array of action values - array of size action_size\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = Input(shape=(self.state_size,), name=\"Input\")\n",
    "        last_layer = inputs\n",
    "        \n",
    "        for i in range(n_hidden_layers):\n",
    "            if verbose:\n",
    "                print(\"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\" % (i + 1, \n",
    "                                                                                hidden_layer_size, \n",
    "                                                                                activation,\n",
    "                                                                                reg_penalty,\n",
    "                                                                                dropout,\n",
    "                                                                               ))\n",
    "            # add dropout, but not on inputs, only between hidden layers\n",
    "            if i and dropout:\n",
    "                last_layer = Dropout(dropout, name = \"Dropout%02d\" % i)(last_layer)\n",
    "            \n",
    "            last_layer = Dense(units = hidden_layer_size, \n",
    "                               activation = activation,\n",
    "                               kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                               kernel_regularizer=keras.regularizers.l2(reg_penalty),\n",
    "                               name = \"Dense%02d\" % i)(last_layer)\n",
    "\n",
    "        outputs = Dense(self.action_size, activation='linear', name = \"Output\")(last_layer)\n",
    "\n",
    "        #model = Model(inputs=input_layer , output=last_layer)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        if verbose:\n",
    "            print(model.summary())\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # append in place\n",
    "        self.memory.loc[self.memory.shape[0]]=[state[0], action, next_state[0], reward, done]\n",
    "            \n",
    "    def train(self, sample_size, start_epoch=0):\n",
    "        # truncate memory\n",
    "        self.memory = self.memory[-self.memory_size:]\n",
    "        # sample sample_size observations from memory\n",
    "        minibatch = self.memory.sample(n=sample_size)\n",
    "        \n",
    "        # target is our best estimate of value of each action\n",
    "        X_fit = np.concatenate(minibatch['state'].values)\n",
    "        X_fit = X_fit.reshape((sample_size, self.state_size))\n",
    "        Y_pred = self.model.predict(X_fit)\n",
    "\n",
    "        # we don't just fit model against model's own prediction, that would get us nowhere\n",
    "        # we improve the target by what we learned about the action we actually took\n",
    "        # value is reward obtained + predicted value of the observed next state\n",
    "        minibatch['target_observed'] = minibatch['reward']\n",
    "        # if done, target is the reward \n",
    "        # reward by gym env is only 1 for each timestep of survival\n",
    "        # but we also added a reward of -10 on failure\n",
    "        # if not done, add gamma discount rate * Q-value prediction for the observed next state\n",
    "        not_done = minibatch.loc[minibatch['done'] == False]\n",
    "        X_observed = np.concatenate(not_done['next_state'].values)\n",
    "        X_observed = X_observed.reshape((not_done.shape[0], self.state_size))\n",
    "        # run all predictions at once\n",
    "        # iterates faster but does not train after each prediction\n",
    "        y_observed_pred = np.amax(self.model.predict(X_observed), axis=1)\n",
    "        minibatch.loc[minibatch['done'] == False, 'target_observed'] += self.gamma * y_observed_pred\n",
    "        # vectorized vlookup - update y_pred column specified by action using target_observed\n",
    "        np.put_along_axis(Y_pred, \n",
    "                          minibatch['action'].astype(int).values.reshape(sample_size,1), \n",
    "                          minibatch['target_observed'].values.reshape(sample_size,1),\n",
    "                          axis=1)\n",
    "        # fit model against improved target\n",
    "        # arbitrary 8 batch size to reduce variance a little and speed up fit\n",
    "        self.model.fit(X_fit, Y_pred, \n",
    "                       epochs=1, initial_epoch=start_epoch,\n",
    "                       batch_size=self.train_batch_size, \n",
    "                       verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def increment_time(self):\n",
    "        self.timestep +=1\n",
    "\n",
    "    def score_episode(self, e, n_episodes):\n",
    "        self.save_score()\n",
    "        avglen=min(len(self.results), self.save_interval)\n",
    "        print(\"{} episode {}: {}/{}, score: {}, {}-episode avg: {:.1f} epsilon: {:.02} Memory: {}        \"\n",
    "              .format(time.strftime(\"%H:%M:%S\"), len(self.results), e+1, n_episodes, self.timestep, \n",
    "                      avglen, sum(self.results[-avglen:])/avglen, self.epsilon, memusage()),\n",
    "              end=\"\\r\", flush=False)\n",
    "        \n",
    "    def save_score(self):\n",
    "        self.results.append(self.timestep)    \n",
    "    \n",
    "    def load(self, filename, memory=True):\n",
    "        self.model = load_model(\"%s.h5\" % filename)\n",
    "        pickledict = pickle.load(open( \"%s.p\" % filename, \"rb\"))\n",
    "        self.memory = pickledict['memory']\n",
    "        self.results = pickledict['results']\n",
    "        self.epsilon = pickledict['epsilon']\n",
    "        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(self.results),\n",
    "                                                                      len(self.memory),\n",
    "                                                                      self.epsilon))\n",
    "\n",
    "    def save(self, pathname, memory=True):\n",
    "        fullname = \"%s%04d\" % (pathname, len(self.results))\n",
    "        self.model.save(\"%s.h5\" % fullname)\n",
    "        pickledict = {\n",
    "            'memory': self.memory,\n",
    "            'results': self.results,\n",
    "            'epsilon': self.epsilon,\n",
    "        }\n",
    "        pickle.dump( pickledict, open( \"%s.p\" % fullname, \"wb\" ) )\n",
    "        #print(\"saved model to %s\" % fullname)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druce/anaconda3/envs/python36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning:\n",
      "\n",
      "Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "sample_size = 128\n",
    "max_timesteps = 500\n",
    "n_episodes = 400\n",
    "win_reward = 10\n",
    "\n",
    "output_dir = 'model_output/cartpole/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 size 16, relu, reg_penalty 0.00100000, dropout 0.068\n",
      "layer 2 size 16, relu, reg_penalty 0.00100000, dropout 0.068\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense00 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "Dropout01 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "Dense01 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load earlier model\n",
    "# start_epoch=400\n",
    "# loadmodel = '%04d' % start_epoch\n",
    "# agent.load(output_dir + 'model_' + loadmodel)\n",
    "# n_episodes = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'262.0 MB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','K','M','G','T','P','E','Z']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Y', suffix)\n",
    "\n",
    "def memusage():\n",
    "    return sizeof_fmt(int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "\n",
    "memusage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:13:01 episode 6: 6/400, score: 27, 6-episode avg: 18.7 epsilon: 1.0 Memory: 262.7 MB        \r"
     ]
    }
   ],
   "source": [
    "# run faster without rendering\n",
    "RENDER=False\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    agent.reset()\n",
    "    done = False\n",
    "    \n",
    "    # run an episode\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # should get extra reward for max + not done vs. max + done\n",
    "        if done and agent.timestep == (max_timesteps -1):\n",
    "            reward += win_reward \n",
    "            \n",
    "        next_state = next_state.reshape([1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        agent.increment_time()\n",
    "    \n",
    "    # after episode\n",
    "    agent.score_episode(e, n_episodes)\n",
    "    \n",
    "    # train\n",
    "    if len(agent.memory) > sample_size*2:\n",
    "        agent.train(max(sample_size, int(agent.memory.shape[0] *0.05)))\n",
    "\n",
    "    # save every so often\n",
    "    if e and (e+1) % agent.save_interval == 0:\n",
    "        agent.save(output_dir + \"model_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'timesteps': agent.results})\n",
    "df['avg'] = df['timesteps'].rolling(10).mean() \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart timesteps vs. episodes\n",
    "def rlplot(agent):\n",
    "    df = pd.DataFrame({'timesteps': agent.results})\n",
    "    df['avg'] = df['timesteps'].rolling(10).mean() \n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df.index, \n",
    "                             y=df['timesteps'],\n",
    "                             mode='markers',\n",
    "                             name='timesteps',\n",
    "                             marker=dict(\n",
    "                                 color='mediumblue',\n",
    "                                 size=4,\n",
    "                             ),\n",
    "                            ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df.index, \n",
    "                             y=df['avg'],\n",
    "                             mode='lines',\n",
    "                             line_width=3,\n",
    "                             name='moving average'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title= dict(text='Cartpole DQN Agent Training Progress',\n",
    "                    x=0.5,\n",
    "                    xanchor='center'),\n",
    "        xaxis=dict(\n",
    "            title=\"Episodes\",\n",
    "            linecolor='black',\n",
    "            linewidth=1,\n",
    "            mirror=True\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Completed Timesteps\",\n",
    "            linecolor='black',\n",
    "            linewidth=1,\n",
    "            mirror=True\n",
    "        ),\n",
    "        legend=go.layout.Legend(\n",
    "            x=0.01,\n",
    "            y=0.99,\n",
    "            traceorder=\"normal\",\n",
    "            font=dict(\n",
    "                family=\"sans-serif\",\n",
    "                size=12,\n",
    "                color=\"black\"\n",
    "            ),\n",
    "            #bgcolor=\"LightSteelBlue\",\n",
    "            bordercolor=\"Black\",\n",
    "            borderwidth=1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig.show()\n",
    "\n",
    "start_epoch=400\n",
    "loadmodel = '%04d' % start_epoch\n",
    "agent.load(output_dir + 'model_' + loadmodel)\n",
    "rlplot(agent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch=1000\n",
    "loadmodel = '%04d' % start_epoch\n",
    "agent.load(output_dir + 'model_' + loadmodel)\n",
    "rlplot(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training as above does well up to a point but not very stable\n",
    "# sometimes performance goes off a cliff esp with more complex NNs like 2x32\n",
    "# continuing to train sometimes results in forgetting what it learned\n",
    "# also on my machine tensorflow leaks memory, can't train long without restarting\n",
    "# trained repeatedly, when it fell off a cliff restarted using best previous model\n",
    "# early stopping after achieving a model that wins many times in a row,\n",
    "# saved best model, run it here without epsilon random exploration, or training\n",
    "\n",
    "agent.load('good_new')\n",
    "agent.epsilon = 0.0\n",
    "print(agent.model.summary())\n",
    "RENDER=True\n",
    "\n",
    "for e in range(10):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    agent.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = state.reshape([1, state_size])\n",
    "        agent.increment_time()\n",
    "\n",
    "    agent.score_episode(e, n_episodes)\n",
    "\n",
    "    # don't train or save after each episode\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE is a policy gradient method\n",
    "# 2 changes vs. DQN\n",
    "# 1) Monte Carlo instead of temporal difference learning:\n",
    "#    after each episode, compute rewards for trajectory runout\n",
    "#    train on the full last episode\n",
    "#    then throw it away (no resampling history)\n",
    "# 2) Use logistic regression first instead of deep NN\n",
    "#    DQN trains against estimate of Q value\n",
    "#    REINFORCE trains against reward\n",
    "#    To update logistic regression theta\n",
    "#      compute gradients of all logistic function (sigmoid) outputs w.r.t. thetas\n",
    "#      compute discounted reward for each observation\n",
    "#      for each action compute average gradient weighted by reward (gradient of average reward wrt theta)\n",
    "#      update each thetas by that amount times learning rate\n",
    "\n",
    "# only 4 params, runs fast and solves consistently after about 500 episodes\n",
    "# logistic regression only\n",
    "# could add hidden layers to make NN but would need to backprop the gradient\n",
    "# tried to use keras NN but the loss function is tricky, not quite working\n",
    "\n",
    "# https://mcneela.github.io/math/2018/04/18/A-Tutorial-on-the-REINFORCE-Algorithm.html\n",
    "# https://karpathy.github.io/2016/05/31/rl/\n",
    "# code mostly from\n",
    "# https://github.com/jklaise/personal_website/blob/master/notebooks/rl_policy_gradients.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticAgent:\n",
    "    \n",
    "    def __init__(self, theta, learning_rate, discount_rate):\n",
    "        \"\"\"Initialize parameter vector theta, learning rate and discount_rate\"\"\"\n",
    "        self.theta = theta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.save_interval=10\n",
    "        self.results=[]\n",
    "        \n",
    "    def logistic(self, y):\n",
    "        \"\"\"logistic function, squash -infinity to +infinity to prob between 0 and 1\"\"\"\n",
    "        return 1/(1 + math.exp(-y))\n",
    "    \n",
    "    def act(self, x):\n",
    "        \"\"\"predict probas using theta, sample an action from probabilities\"\"\"\n",
    "        y = x @ self.theta\n",
    "        prob0 = self.logistic(y)\n",
    "        probs = np.array([prob0, 1-prob0])\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "        return action, probs[action]\n",
    "    \n",
    "    def grad_log_p(self, x):\n",
    "        \"\"\"calculate gradient vector of log-probas\"\"\"\n",
    "        y = x @ self.theta        \n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "        return grad_log_p0, grad_log_p1\n",
    "        \n",
    "    def discount_rewards(self, rewards):\n",
    "        \"\"\"calculate discounted rewards\"\"\"\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.discount_rate + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train(self, rewards, obs, actions):\n",
    "        \"\"\"update thetas based on gradients, discounted rewards, learning rate\"\"\"\n",
    "        # calculate gradients for each action you actually took\n",
    "        # how much to adjust theta to increase prob of that action\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "\n",
    "        # calculate discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        # standardize\n",
    "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / np.std(discounted_rewards)\n",
    "        \n",
    "        # gradients times discounted rewards\n",
    "        # average gradient over all obs weighted by reward\n",
    "        # how much to update theta to increase reward\n",
    "        update_target = grad_log_p.T @ discounted_rewards\n",
    "\n",
    "        # update theta\n",
    "        self.theta += self.learning_rate*update_target\n",
    "\n",
    "    def save_score(self, reward):\n",
    "        self.results.append(reward)    \n",
    "        \n",
    "    def score_episode(self, e, n_episodes):\n",
    "        avglen=min(len(self.results), self.save_interval)\n",
    "        print(\"{} episode {}/{}:, score: {}, {}-episode avg: {:.1f}        \"\n",
    "              .format(time.strftime(\"%H:%M:%S\"), e+1, n_episodes, self.results[-1], \n",
    "                      avglen, sum(self.results[-avglen:])/avglen),\n",
    "              end=\"\\r\", flush=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEPS = 500\n",
    "WIN_REWARD=10\n",
    "\n",
    "def run_episode(env, agent, render=False):\n",
    "    \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        observations.append(observation)\n",
    "        \n",
    "        action, prob = agent.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # should get extra reward for max + not done vs. max + done\n",
    "        if done and totalreward == (MAX_TIMESTEPS -1):\n",
    "            reward += WIN_REWARD \n",
    "         \n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "            \n",
    "    agent.save_score(totalreward)\n",
    "    \n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES=1000\n",
    "RENDER=False\n",
    "\n",
    "def run_experiment(theta, learning_rate, discount_rate, AgentClass, MAX_EPISODES=1000, seed=None):\n",
    "    \n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1')\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    episode_rewards = []\n",
    "    agent = AgentClass(theta, learning_rate, discount_rate)\n",
    "    \n",
    "    # train until MAX_EPISODES\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        # run a single episode\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, agent, render=RENDER)\n",
    "                \n",
    "        # keep track of episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # update policy\n",
    "        agent.train(rewards, observations, actions)\n",
    "        \n",
    "        agent.score_episode(i, MAX_EPISODES)\n",
    "                \n",
    "    return episode_rewards, agent\n",
    "\n",
    "\n",
    "episode_rewards, agent = run_experiment(theta=np.random.rand(4),\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_rate=0.975,\n",
    "                                        AgentClass=LogisticAgent,\n",
    "                                        MAX_EPISODES=MAX_EPISODES,\n",
    "                                        seed=GLOBAL_SEED,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view trained agent\n",
    "env = gym.make('CartPole-v1')\n",
    "total_reward, rewards, observations, actions, probs = run_episode(env, agent, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlplot(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras REINFORCE policy gradient method\n",
    "\n",
    "class REINFORCE_Agent:\n",
    "    def __init__(self, state_size=4, action_size=2, learning_rate=0.0005, discount_rate=0.98,\n",
    "                 n_hidden_layers=2, hidden_layer_size=16, activation='relu', reg_penalty=0, dropout=0,\n",
    "                 verbose=True):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_space = list(range(action_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        \n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.activation=activation\n",
    "        self.reg_penalty=reg_penalty\n",
    "        self.dropout=dropout\n",
    "        self.verbose=verbose        \n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy, self.predict = self.build_models()\n",
    "        self.results = []\n",
    "        self.timestep=0\n",
    "        self.save_interval=10\n",
    "        \n",
    "    def build_models(self):\n",
    "        \n",
    "        def custom_loss(y_true, y_pred):\n",
    "            y_pred_clip = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_likelihood = y_true*K.log(y_pred_clip)\n",
    "\n",
    "            return K.sum(-log_likelihood*discounted_rewards)\n",
    "        \n",
    "        inputs = Input(shape=(self.state_size,), name=\"Input\")\n",
    "        discounted_rewards = Input(shape=(1,), name=\"Discounted_rewards\")\n",
    "        last_layer = inputs\n",
    "                \n",
    "        for i in range(self.n_hidden_layers):\n",
    "            if self.verbose:\n",
    "                print(\"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\" % (i + 1, \n",
    "                                                                                self.hidden_layer_size, \n",
    "                                                                                self.activation,\n",
    "                                                                                self.reg_penalty,\n",
    "                                                                                self.dropout,\n",
    "                                                                               ))\n",
    "            # add dropout, but not on inputs, only between hidden layers\n",
    "            if i and self.dropout:\n",
    "                last_layer = Dropout(self.dropout, name = \"Dropout%02d\" % i)(last_layer)\n",
    "            \n",
    "            last_layer = Dense(units = self.hidden_layer_size, \n",
    "                               activation = self.activation,\n",
    "                               kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                               kernel_regularizer=keras.regularizers.l2(self.reg_penalty),\n",
    "                               name = \"Dense%02d\" % i)(last_layer)\n",
    "\n",
    "        outputs = Dense(self.action_size, activation='softmax', name = \"Output\")(last_layer)\n",
    "\n",
    "        train_model = Model(inputs=[inputs, discounted_rewards], outputs=[outputs])\n",
    "        train_model.compile(optimizer=Adam(lr=self.learning_rate), loss=custom_loss)\n",
    "\n",
    "        predict_model = Model(inputs=[inputs], outputs=[outputs])\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(predict_model.summary())\n",
    "\n",
    "        return train_model, predict_model\n",
    "    \n",
    "    def act(self, state):\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        return action\n",
    "    \n",
    "    def remember(self, state, action, reward):\n",
    "        self.state_memory.append(state[0])\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def train(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        actions = np.zeros([len(action_memory), self.action_size])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "        \n",
    "        discounted_rewards = np.zeros_like(reward_memory)\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(len(reward_memory))):\n",
    "            cumulative_rewards = cumulative_rewards * self.discount_rate + reward_memory[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "        \n",
    "        # standardize\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards) if np.std(discounted_rewards) > 0 else 1\n",
    "\n",
    "        # train\n",
    "        cost = self.policy.train_on_batch([state_memory, discounted_rewards], actions)\n",
    "\n",
    "        # truncate memory\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def increment_time(self):\n",
    "        self.timestep +=1\n",
    "\n",
    "    def score_episode(self, e, n_episodes):\n",
    "        self.save_score()\n",
    "        avglen=min(len(self.results), self.save_interval)\n",
    "        print(\"{} episode {}/{}:, score: {}, {}-episode avg: {:.1f} Memory: {}        \"\n",
    "              .format(time.strftime(\"%H:%M:%S\"), e+1, n_episodes, self.timestep, \n",
    "                      avglen, sum(self.results[-avglen:])/avglen, memusage()),\n",
    "              end=\"\\r\", flush=False)\n",
    "        \n",
    "    def save_score(self):\n",
    "        self.results.append(self.timestep)    \n",
    "    \n",
    "    def load(self, filename, memory=True):\n",
    "        self.model = load_model(\"%s.h5\" % filename)\n",
    "        pickledict = pickle.load(open( \"%s.p\" % filename, \"rb\"))\n",
    "        self.memory = pickledict['memory']\n",
    "        self.results = pickledict['results']\n",
    "        self.epsilon = pickledict['epsilon']\n",
    "        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(self.results),\n",
    "                                                                      len(self.memory),\n",
    "                                                                      self.epsilon))\n",
    "\n",
    "    def save(self, pathname, memory=True):\n",
    "        fullname = \"%s%04d\" % (pathname, len(self.results))\n",
    "        self.policy.save(\"%s_train.h5\" % fullname)        \n",
    "        self.predict.save(\"%s_predict.h5\" % fullname)        \n",
    "        pickledict = {\n",
    "            'state_memory': self.state_memory,\n",
    "            'action_memory': self.action_memory,\n",
    "            'reward_memory': self.reward_memory,\n",
    "            'results': self.results,\n",
    "        }\n",
    "        pickle.dump( pickledict, open( \"%s.p\" % fullname, \"wb\" ) )\n",
    "        #print(\"saved model to %s\" % fullname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEPS = 500\n",
    "N_EPISODES = 1000\n",
    "WIN_REWARD = 10\n",
    "# run faster without rendering\n",
    "RENDER=False\n",
    "\n",
    "#https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = REINFORCE_Agent(state_size=state_size, action_size=action_size, learning_rate=0.0005, discount_rate=0.98,)\n",
    "\n",
    "output_dir = 'model_output/cartpole/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# train\n",
    "\n",
    "for e in range(N_EPISODES):\n",
    "    agent.reset()\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    \n",
    "    # run an episode\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # should get extra reward for max + not done vs. max + done\n",
    "        if done and agent.timestep == (MAX_TIMESTEPS -1):\n",
    "            reward += WIN_REWARD \n",
    "            \n",
    "        next_state = next_state.reshape([1, state_size])\n",
    "        agent.remember(state, action, reward)\n",
    "        state = next_state\n",
    "        agent.increment_time()\n",
    "    \n",
    "    # after episode\n",
    "    agent.score_episode(e, N_EPISODES)\n",
    "    \n",
    "    # train\n",
    "    agent.train()\n",
    "\n",
    "    # save every so often\n",
    "    if e and (e+1) % agent.save_interval == 0:\n",
    "        agent.save(output_dir + \"reinforce_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlplot(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view it in action\n",
    "\n",
    "agent.reset()\n",
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "\n",
    "# run an episode\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
