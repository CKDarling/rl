{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on code from Deep Learning Illustrated by Jon Krohn\n",
    "# https://www.amazon.com/Deep-Learning-Illustrated-Intelligence-Addison-Wesley/dp/0135116694\n",
    "# in turn based on bit.ly/keonDQN\n",
    "\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# to track down memory leak\n",
    "import resource\n",
    "from pympler import tracker\n",
    "\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# requires python 3.6\n",
    "# conda install -c akode gym\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000) # double-ended queue\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        self.memory_tracker = tracker.SummaryTracker()\n",
    "        \n",
    "    def build_model(self,\n",
    "                    n_hidden_layers=2, \n",
    "                    hidden_layer_size=32, \n",
    "                    activation='relu',\n",
    "                    reg_penalty=0.0,\n",
    "                    dropout=False,\n",
    "                    verbose=True\n",
    "                   ):\n",
    "        \"\"\"return keras NN model per inputs\n",
    "        input is a state - array of size state_size\n",
    "        output is an array of action values - array of size action_size\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(n_hidden_layers):\n",
    "            if verbose:\n",
    "                print(\"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\" % (i + 1, \n",
    "                                                                                hidden_layer_size, \n",
    "                                                                                activation,\n",
    "                                                                                reg_penalty,\n",
    "                                                                                dropout,\n",
    "                                                                               ))\n",
    "            # add dropout, but not on inputs, only between hidden layers\n",
    "            if i and dropout:\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "            if i==0: # first layer, specify input shape\n",
    "                model.add(Dense(input_shape=(state_size,),\n",
    "                                units = hidden_layer_size, \n",
    "                                activation = activation,\n",
    "                                kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                                kernel_regularizer=keras.regularizers.l2(reg_penalty),\n",
    "                                name = \"Dense%02d\" % i))\n",
    "            else: #use implicit input shape\n",
    "                model.add(Dense(units = hidden_layer_size, \n",
    "                                activation = activation,\n",
    "                                kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                                kernel_regularizer=keras.regularizers.l2(reg_penalty),\n",
    "                                name = \"Dense%02d\" % i))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear', name=\"Output\"))\n",
    "\n",
    "        if verbose:\n",
    "            print(model.summary())\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        pdb.set_trace()\n",
    "        # get batch_size observations from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # target is reward plus current Q prediction of value of action\n",
    "            target_fit = self.model.predict(state)\n",
    "            # but we don't want to fit against our own prediction\n",
    "            # we improve the target by what we observed about the action we took\n",
    "            target_actual = reward\n",
    "            if not done:\n",
    "                # add discount factor * value of predicted next state\n",
    "                # self.model.predict(next_state).max()\n",
    "                target_actual += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_fit[0][action] = target_actual\n",
    "            self.model.fit(state, target_fit, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        # self.memory_tracker.print_diff()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def load(self, filename):\n",
    "        with open('%s.json' % filename, 'r') as json_file:\n",
    "            self.model = model_from_json(json_file.read())\n",
    "        self.model.load_weights(\"%s.h5\" % filename)\n",
    "\n",
    "    def save(self, filename):\n",
    "        # serialize model to JSON\n",
    "        with open(\"%s.json\" % filename, \"w\") as json_file:\n",
    "            json_file.write(self.model.to_json())\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights(\"%s.h5\" % filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gym.openai.com/envs/CartPole-v0/\n",
    "env = gym.make('CartPole-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 32\n",
    "n_episodes=1000\n",
    "output_dir = 'model_output/cartpole/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    print ('Memory usage: %s (kb)' % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    timesteps = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = next_state.reshape([1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"{} episode: {}/{}, score: {}, epsilon: {:.02}\"\n",
    "                  .format(time.strftime(\"%H:%M:%S\"), e, n_episodes, timesteps, agent.epsilon))\n",
    "        timesteps +=1\n",
    "            \n",
    "    if len(agent.memory) > batch_size:\n",
    "        #pdb.set_trace()\n",
    "        agent.train(batch_size)\n",
    "    if e % 10 == 0:\n",
    "        agent.save(output_dir + \"model_%.04d\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
