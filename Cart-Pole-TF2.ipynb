{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to log experiments\n",
    "# sort of cloud Tensorboard to \n",
    "# https://www.comet.ml/emergent-dynamics/projects\n",
    "\n",
    "from comet_ml import Experiment\n",
    "COMET_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.1.0\n",
      "Keras 2.2.4-tf\n",
      "gym 0.15.6\n",
      "plotly 4.5.0\n",
      "pandas 1.0.1\n",
      "numpy 1.18.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import resource\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# requires python 3.6\n",
    "# conda install -c akode gym\n",
    "import gym\n",
    "\n",
    "# set seeds for reproducibility\n",
    "# np.random.uniform(0,10000) 4465\n",
    "GLOBAL_SEED = 4465\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "#tf.random.set_seed(GLOBAL_SEED)\n",
    "\n",
    "print(\"TensorFlow %s\" % tf.__version__)\n",
    "print(\"Keras %s\" % keras.__version__)\n",
    "print(\"gym %s\" % gym.__version__)\n",
    "print(\"plotly %s\" % plotly.__version__)\n",
    "print(\"pandas %s\" % pd.__version__)\n",
    "print(\"numpy %s\" % np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = 2\n",
    "\n",
    "MAX_TIMESTEPS = 500\n",
    "N_EPISODES = 2000\n",
    "WIN_REWARD = 100\n",
    "DISCOUNT_RATE = 0.98\n",
    "SAMPLE_SIZE = 128\n",
    "BATCH_SIZE = 1\n",
    "OUTPUT_DIR = 'model_output/cartpole/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "N_HIDDEN_LAYERS = 1\n",
    "HIDDEN_LAYER_SIZE = 32\n",
    "\n",
    "CARTPOLE_EPISODES = 200\n",
    "LUNARLANDER_EPISODES = 500\n",
    "RENDER = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"discount_rate\": DISCOUNT_RATE,\n",
    "    \"n_hidden_layers\": N_HIDDEN_LAYERS,\n",
    "    \"hidden_layer_size\": HIDDEN_LAYER_SIZE,\n",
    "}\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment.log_parameters(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'359.1 KB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show memory usage (some versions of TensorFlow gave memory issues)\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    \"\"\"given memory as int format as memory units eg KB\"\"\"\n",
    "    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Y', suffix)\n",
    "\n",
    "def memusage():\n",
    "    \"\"\"print memory usage\"\"\"\n",
    "    return sizeof_fmt(int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "\n",
    "memusage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"abstract base class for agents\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, filename=\"model\",\n",
    "                 *args, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.filename = filename\n",
    "        self.timestep = 0\n",
    "        self.total_reward = 0\n",
    "        self.save_interval = 10\n",
    "        self.max_score = -9999\n",
    "        self.max_avg = -9999\n",
    "        self.state = None\n",
    "\n",
    "    def run_episode(self, env, render=RENDER):\n",
    "        \"\"\"run a full episode\"\"\"\n",
    "\n",
    "        self.reset()\n",
    "        self.state = env.reset()\n",
    "        self.done = False\n",
    "\n",
    "        while not self.done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            self.action = self.act(self.state.reshape([1, self.state_size]))\n",
    "            self.next_state, self.reward, self.done, _ = env.step(self.action)\n",
    "            self.total_reward += self.reward\n",
    "            # should get extra reward for max + not done vs. max + done\n",
    "            if self.done and self.timestep == (MAX_TIMESTEPS - 1):\n",
    "                self.reward += WIN_REWARD\n",
    "\n",
    "            self.remember()\n",
    "            self.state = self.next_state\n",
    "            self.increment_time()\n",
    "            \n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        self.train()    \n",
    "        \n",
    "    def build_model(self, *args, **kwargs):\n",
    "        \"\"\"build a model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"reset agent for start of an episode\"\"\"\n",
    "        self.timestep = 0\n",
    "        self.total_reward = 0\n",
    "        self.max_score = -9999\n",
    "        self.max_avg = -9999\n",
    "\n",
    "    def act(self, *args, **kwargs):\n",
    "        \"\"\"pick an action using model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def increment_time(self):\n",
    "        \"\"\"increment timestep counter\"\"\"\n",
    "        self.timestep += 1\n",
    "\n",
    "    def remember(self, *args, **kwargs):\n",
    "        \"\"\"store the states and rewards needed to fit the model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        \"\"\"train the model on experience stored by remember\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_score(self):\n",
    "        \"\"\"save score of each episode\"\"\"\n",
    "        self.results.append(self.total_reward)\n",
    "\n",
    "    def score_episode(self, episode_num, n_episodes):\n",
    "        \"\"\"output results and save\"\"\"\n",
    "        self.save_score()\n",
    "        avglen = min(episode_num+1, self.save_interval)\n",
    "        self.avgscore = sum(self.results[-avglen:])/avglen\n",
    "        self.max_score = max(self.total_reward, self.max_score)\n",
    "        self.max_avg = max(self.avgscore, self.max_avg)\n",
    "\n",
    "        formatstr = \"{} episode {}/{}:, score: {} (max {}), {}-episode avg: {:.1f} (max {:.1f}) Memory: {}             \"\n",
    "        print(formatstr.format(time.strftime(\"%H:%M:%S\"), episode_num,\n",
    "                               n_episodes, self.total_reward, self.max_score, avglen,\n",
    "                               self.avgscore, self.max_avg, memusage()),\n",
    "              end=\"\\r\", flush=False)\n",
    "\n",
    "    def view(self, render=True):\n",
    "        \"\"\"Run an episode without training, with rendering\"\"\"\n",
    "        \n",
    "        if not render:\n",
    "            return\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        done = False\n",
    "\n",
    "        # run an episode\n",
    "        self.timestep = 0\n",
    "        r = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = self.act(state, argmax=True)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            r += reward\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            self.timestep += 1\n",
    "        env.render()\n",
    "        print(r)\n",
    "        env.close()\n",
    "        return self.timestep\n",
    "\n",
    "    def rlplot(self, title='Cartpole Agent Training Progress'):\n",
    "        \"\"\"plot training progress\"\"\"\n",
    "        df = pd.DataFrame({'timesteps': self.results})\n",
    "        df['avg'] = df['timesteps'].rolling(10).mean()\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df.index,\n",
    "                                 y=df['timesteps'],\n",
    "                                 mode='markers',\n",
    "                                 name='timesteps',\n",
    "                                 marker=dict(\n",
    "                                     color='mediumblue',\n",
    "                                     size=4,\n",
    "                                 ),\n",
    "                                ))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=df.index,\n",
    "                                 y=df['avg'],\n",
    "                                 mode='lines',\n",
    "                                 line_width=3,\n",
    "                                 name='moving average'))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=dict(text=title,\n",
    "                       x=0.5,\n",
    "                       xanchor='center'),\n",
    "            xaxis=dict(\n",
    "                title=\"Episodes\",\n",
    "                linecolor='black',\n",
    "                linewidth=1,\n",
    "                mirror=True\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=\"Total Reward per Episode\",\n",
    "                linecolor='black',\n",
    "                linewidth=1,\n",
    "                mirror=True\n",
    "            ),\n",
    "            legend=go.layout.Legend(\n",
    "                x=0.01,\n",
    "                y=0.99,\n",
    "                traceorder=\"normal\",\n",
    "                font=dict(\n",
    "                    family=\"sans-serif\",\n",
    "                    size=12,\n",
    "                    color=\"black\"\n",
    "                ),\n",
    "                #bgcolor=\"LightSteelBlue\",\n",
    "                bordercolor=\"Black\",\n",
    "                borderwidth=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return fig.show()\n",
    "    \n",
    "    def save_agent(self, *args, **kwargs):\n",
    "        \"\"\"save agent to disk\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_agent(*args, **kwargs):\n",
    "        \"\"\"load agent from disk\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network\n",
    "# based on Deep Learning Illustrated by Jon Krohn\n",
    "# https://www.amazon.com/Deep-Learning-Illustrated-Intelligence-Addison-Wesley/dp/0135116694\n",
    "# in turn based on bit.ly/keonDQN\n",
    "# to speed training, put memory in dataframe and train in batch\n",
    "# still slow and so-so performing\n",
    "\n",
    "class PolicyModel(Model):\n",
    "            \n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "                 hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                 activation='relu',\n",
    "                 reg_penalty=0.001,\n",
    "                 dropout=0.0625,\n",
    "                 verbose=True\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation = activation\n",
    "        self.reg_penalty = reg_penalty\n",
    "        self.dropout = dropout\n",
    "        self.verbose = verbose\n",
    "                \n",
    "        self.rllayers = []\n",
    "\n",
    "        for i in range(n_hidden_layers):\n",
    "            if verbose:\n",
    "                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n",
    "                print(formatstr % (i + 1, hidden_layer_size, activation, reg_penalty, dropout))\n",
    "                \n",
    "            # add dropout, only between hidden layers\n",
    "            if i and dropout:\n",
    "                self.rllayers.append(Dropout(dropout, name=\"Dropout%02d\" % i))\n",
    "                \n",
    "            self.rllayers.append(Dense(hidden_layer_size, activation=activation, name=\"Dense%02d\" % i))\n",
    "\n",
    "        self.rllayers.append(Dense(self.action_size, activation='linear', name=\"Output\"))\n",
    "\n",
    "    def call(self, x):\n",
    "        # Forward pass\n",
    "        for layer in self.rllayers:\n",
    "            x = layer(x)\n",
    "        return x  \n",
    "\n",
    "    def get_config(self):\n",
    "        return {'state_size': self.state_size,\n",
    "                'action_size': self.action_size,\n",
    "                'n_hidden_layers': self.n_hidden_layers,\n",
    "                'hidden_layer_size': self.hidden_layer_size,\n",
    "                'activation': self.activation,\n",
    "                'reg_penalty': self.reg_penalty,\n",
    "                'dropout': self.dropout,\n",
    "                'verbose': self.verbose,\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 size 32, relu, reg_penalty 0.00100000, dropout 0.062\n",
      "[0.00832242 0.02016691 0.01829497 0.01116533]\n",
      "[[0.00832242 0.02016691 0.01829497 0.01116533]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/gym/logger.py:30: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00032636,  0.00070215]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the model does something\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "testmodel = PolicyModel(env.observation_space.shape[0], env.action_space.n)\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "# make n x action_size array\n",
    "obs = obs[None, :]\n",
    "print(obs)\n",
    "z = testmodel.predict(obs)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02943283, -0.01134082]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.predict(np.random.uniform(size=(1, env.observation_space.shape[0]))/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent(Agent):\n",
    "    def __init__(self, state_size, action_size, filename=\"dqn\",\n",
    "                 discount_rate=DISCOUNT_RATE,\n",
    "                 learning_rate=None,\n",
    "                 epsilon=1.0,\n",
    "                 epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01,\n",
    "                 n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "                 hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                ):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.filename = filename\n",
    "        self.discount_rate = discount_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = PolicyModel(state_size, action_size,\n",
    "                                 n_hidden_layers=n_hidden_layers,\n",
    "                                 hidden_layer_size=hidden_layer_size)\n",
    "        \n",
    "        self.loss_function = keras.losses.MSE\n",
    "        self.optimizer = keras.optimizers.Adam()        \n",
    "\n",
    "        self.memory = pd.DataFrame(columns=[\"state\", \"action\", \"next_state\",\n",
    "                                            \"reward\", \"done\"])\n",
    "        self.memory_size = 20000\n",
    "        self.results = []\n",
    "        self.train_batch_size = BATCH_SIZE\n",
    "        self.timestep = 0\n",
    "        self.save_interval = 10\n",
    "        \n",
    "        self.max_score = 0\n",
    "        self.max_avg = 0\n",
    "\n",
    "    def remember(self):\n",
    "        \"\"\"store the states and rewards needed to fit the model\"\"\"\n",
    "        # append in place\n",
    "        self.memory.loc[self.memory.shape[0]] = [self.state,\n",
    "                                                 self.action,\n",
    "                                                 self.next_state,\n",
    "                                                 self.reward,\n",
    "                                                 self.done]\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, X, Y):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(BATCH_SIZE)\n",
    "        for X_batch, Y_batch in train_ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.model(X_batch)\n",
    "                loss = self.loss_function(Y_batch, predictions)\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"train the model on experience stored by remember\n",
    "        warning: strange loop-y magic\"\"\"\n",
    "\n",
    "        # need at least SAMPLE_SIZE observations\n",
    "        if self.memory.shape[0] < SAMPLE_SIZE:\n",
    "            return\n",
    "\n",
    "        # truncate memory\n",
    "        self.memory = self.memory[-self.memory_size:]\n",
    "        # sample sample_size observations from memory\n",
    "        minibatch = self.memory.sample(n=SAMPLE_SIZE)\n",
    "\n",
    "        # target is our best estimate of value of each action\n",
    "        X_fit = np.concatenate(minibatch['state'].values)\n",
    "        X_fit = X_fit.reshape((SAMPLE_SIZE, self.state_size))\n",
    "        Y_pred = self.model.predict(X_fit)\n",
    "\n",
    "        # we don't just fit model against model's own prediction, gets us nowhere\n",
    "        # we improve the target by what we learned about the action we actually took\n",
    "        # value is reward obtained + predicted value of the observed next state\n",
    "        # this is the strange loop-y magic of temporal difference RL\n",
    "        minibatch['target_observed'] = minibatch['reward']\n",
    "        # if done, target is the reward\n",
    "        # reward by gym env is only 1 for each timestep of survival\n",
    "        # (but we also added a reward of for reaching the end successfully)\n",
    "        # if not done, add discount_rate  * Q-value prediction for  observed next state\n",
    "        not_done = minibatch.loc[minibatch['done'] == False]\n",
    "        X_observed = np.concatenate(not_done['next_state'].values)\n",
    "        X_observed = X_observed.reshape((not_done.shape[0], self.state_size))\n",
    "        # run all predictions at once\n",
    "        # iterates faster but does not train after each prediction\n",
    "        y_observed_pred = np.amax(self.model.predict(X_observed), axis=1)\n",
    "        minibatch.loc[minibatch['done'] == False, 'target_observed'] \\\n",
    "            += self.discount_rate * y_observed_pred\n",
    "        # vectorized vlookup - update col specified by action with target_observed\n",
    "        np.put_along_axis(Y_pred,\n",
    "                          minibatch['action'].astype(int).values.reshape(SAMPLE_SIZE, 1),\n",
    "                          minibatch['target_observed'].values.reshape(SAMPLE_SIZE, 1),\n",
    "                          axis=1)\n",
    "        # fit model against improved target\n",
    "        self.train_step(tf.convert_to_tensor(X_fit, dtype=tf.float64),\n",
    "                        Y_pred)\n",
    "        # self.model.fit(X_fit, Y_pred\n",
    "                       #epochs=1,\n",
    "                       #batch_size=self.train_batch_size,\n",
    "                       #verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state, argmax=False):\n",
    "        \"\"\"pick an action using model\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'state_size': self.state_size,\n",
    "                'action_size': self.action_size,\n",
    "                'filename': self.filename,\n",
    "                'discount_rate': self.discount_rate,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'epsilon': self.epsilon,\n",
    "                'epsilon_decay': self.epsilon_decay,\n",
    "                'epsilon_min': self.epsilon_min,\n",
    "                'loss_function': self.loss_function,\n",
    "                'optimizer': self.optimizer,\n",
    "                'memory': self.memory,\n",
    "                'memory_size': self.memory_size,\n",
    "                'results': self.results,\n",
    "                'train_batch_size': self.train_batch_size,\n",
    "                'timestep': self.timestep,\n",
    "                'save_interval': self.save_interval,\n",
    "               }\n",
    "\n",
    "    def save_agent(self):\n",
    "        \"\"\"save agent: pickle self and use Keras native save model\"\"\"\n",
    "        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n",
    "        agent_attrs = self.get_config()\n",
    "        model_attrs = self.model.get_config()\n",
    "        self.model.save_weights(\"%s.h5\" % fullname)\n",
    "        pickle.dump({'agent_attrs': agent_attrs, 'model_attrs': model_attrs}, open(\"%s.p\" % fullname, \"wb\"))\n",
    "\n",
    "    def load_agent(filename):\n",
    "        \"\"\"load saved agent\"\"\"\n",
    "        pickledict = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n",
    "        new = DQN_Agent(state_size=pickledict['agent_attrs']['state_size'],\n",
    "                        action_size=pickledict['agent_attrs']['action_size'],\n",
    "                        discount_rate=pickledict['agent_attrs']['discount_rate'],\n",
    "                        learning_rate=pickledict['agent_attrs']['learning_rate'],\n",
    "                        epsilon=pickledict['agent_attrs']['epsilon'],\n",
    "                        epsilon_decay=pickledict['agent_attrs']['epsilon_decay'],\n",
    "                        epsilon_min=pickledict['agent_attrs']['epsilon_min']                        \n",
    "                       )\n",
    "        for name, value in pickledict['agent_attrs'].items():\n",
    "            setattr(new, name, value)\n",
    "        # make a prediction to fix input size\n",
    "        new.model.predict(np.random.uniform(size=(1, new.state_size))/10)\n",
    "            \n",
    "        new.model.load_weights(\"%s.h5\" % filename, by_name=False)           \n",
    "        for name, value in pickledict['model_attrs'].items():\n",
    "            setattr(new.model, name, value)\n",
    "        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(new.results),\n",
    "                                                                      len(new.memory),\n",
    "                                                                      new.epsilon))\n",
    "        return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent, env, n_episodes):\n",
    "    print(\"Start training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        agent.run_episode(env)\n",
    "        agent.score_episode(e, n_episodes)\n",
    "        if COMET_ENABLED:\n",
    "            experiment.log_metrics({\n",
    "                'episode': len(agent.results),\n",
    "                'reward': agent.total_reward,\n",
    "                'avg_reward': agent.avgscore\n",
    "            })\n",
    "        \n",
    "        if e and (e+1) % agent.save_interval == 0:\n",
    "            agent.save_agent()\n",
    "            \n",
    "    print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 size 32, relu, reg_penalty 0.00100000, dropout 0.062\n",
      "Start training: 17:49:15\n",
      "17:50:22 episode 138/200:, score: 41.0 (max 41.0), 10-episode avg: 46.2 (max 46.2) Memory: 1.7 MB               \r"
     ]
    }
   ],
   "source": [
    "# https://gym.openai.com/envs/CartPole-v1/\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment = Experiment(project_name=\"DV_Cartpole_DQN\",\n",
    "                            auto_param_logging=False)\n",
    "\n",
    "N_EPISODES=CARTPOLE_EPISODES\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "agent = DQN_Agent(state_size=env.observation_space.shape[0],\n",
    "                  action_size=env.action_space.n)\n",
    "\n",
    "run_experiment(agent, env, N_EPISODES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMET_ENABLED:\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training progress\n",
    "\n",
    "agent.rlplot(\"DQN Cartpole Agent Training Progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view agent in action\n",
    "# can use early stopping to pick a good model\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "# start_epoch = 1730\n",
    "# loadmodel = '%05d' % start_epoch\n",
    "# agent = DQN_Agent.load_agent(OUTPUT_DIR + 'dqn' + loadmodel)\n",
    "agent.view(render=RENDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train additional episodes from a good model\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "# load old model\n",
    "start_epoch = 520\n",
    "\n",
    "loadmodel = '%05d' % start_epoch\n",
    "agent = DQN_Agent.load_agent(OUTPUT_DIR + 'dqn' + loadmodel)\n",
    "\n",
    "z = 10\n",
    "print(\"Resume training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "for e in range(z):\n",
    "    agent.run_episode(env)\n",
    "    agent.score_episode(e, z)\n",
    "    if e and (e+1) % agent.save_interval == 0:\n",
    "        agent.save_agent()  \n",
    "print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training as above does well up to a point but not very stable.\n",
    "# sometimes performance goes off a cliff esp with more complex NNs like 2x32.\n",
    "# continuing to train sometimes results in forgetting what it learned.\n",
    "# trained repeatedly, when it fell off a cliff restarted using best previous model\n",
    "# used early stopping after achieving a model that wins many times in a row,\n",
    "\n",
    "agent = DQN_Agent.load_agent(\"good\")\n",
    "agent.view(render=RENDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE is a policy gradient method\n",
    "# key changes vs. DQN\n",
    "# 1) Monte Carlo instead of temporal difference learning:\n",
    "#    after each episode, compute rewards for trajectory runout\n",
    "#    train on the full last episode\n",
    "#    then throw it away (no resampling history)\n",
    "# 2) Use pure python logistic regression here instead of deep NN\n",
    "#    logistic regression outputs probabilities\n",
    "#    act by sampling the predicted probabilities for each action\n",
    "#    one action may become strongly favored \n",
    "#    but you always explore the other a nonzero % of time\n",
    "#    so no epsilon\n",
    "# 3) DQN trains against estimate of a Q state-action value\n",
    "#    REINFORCE trains action prob predictions directly against observed rewards\n",
    "#    No expectation of Q action-value is computed\n",
    "#    To update logistic regression theta\n",
    "#      over all observations (i.e. each action taken)\n",
    "#        compute gradient of action prob w.r.t. thetas\n",
    "#        compute standardized discounted reward for each observation (state/action taken)\n",
    "#        compute gradient of average reward over all observations w.r.t theta\n",
    "#        update each theta by that amount times learning rate\n",
    "#    This will tend to update thetas so that\n",
    "#        actions with above-average rewards become more probable\n",
    "#        actions with below-average rewards become less probable\n",
    "# only 4 params, runs fast and solves consistently after about 500 episodes\n",
    "# could add hidden layers for NN but would need to backprop the gradient (or use Keras)\n",
    "# also only supports 2 actions (logistic regression/binary classification)\n",
    "# could add softmax (or use Keras)\n",
    "\n",
    "# https://mcneela.github.io/math/2018/04/18/A-Tutorial-on-the-REINFORCE-Algorithm.html\n",
    "# https://karpathy.github.io/2016/05/31/rl/\n",
    "# code mostly from\n",
    "# https://github.com/jklaise/personal_website/blob/master/notebooks/rl_policy_gradients.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticAgent(Agent):\n",
    "    \"\"\"REINFORCE agent (policy gradient) using logistic regression\"\"\"\n",
    "    def __init__(self, theta, learning_rate, discount_rate, filename='logistic'):\n",
    "        \"\"\"Initialize parameter vector theta, learning rate and discount_rate\"\"\"\n",
    "        self.theta = theta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.save_interval = 10\n",
    "        self.results = []\n",
    "        self.timestep = 0\n",
    "        self.state_size = 4\n",
    "        self.filename = filename\n",
    "\n",
    "        self.max_score = 0\n",
    "        self.max_avg = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"reset agent for start of episode\"\"\"\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "        self.probs = []\n",
    "        self.timestep = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def logistic(self, y):\n",
    "        \"\"\"logistic function, squash -infinity to +infinity to prob between 0 and 1\"\"\"\n",
    "        return 1/(1 + math.exp(-y))\n",
    "\n",
    "    def remember(self):\n",
    "        self.state_history.append(self.state)\n",
    "        self.reward_history.append(self.reward)\n",
    "        self.action_history.append(self.action)\n",
    "\n",
    "    def act(self, X, argmax=False):\n",
    "        \"\"\"predict probas using theta, sample an action from probabilities\"\"\"\n",
    "        # use same calling convention as Keras predict, which expects array X of n states\n",
    "        x = X[0]\n",
    "        y = x @ self.theta\n",
    "        prob0 = self.logistic(y)\n",
    "        probs = np.array([prob0, 1-prob0])\n",
    "        # sample action from predicted probabilities\n",
    "        if argmax:\n",
    "            # for play() choose best action\n",
    "            action = np.argmax(probs)\n",
    "        else:\n",
    "            # for train() sample actions\n",
    "            action = np.random.choice([0, 1], p=probs)\n",
    "        # save prob history\n",
    "        self.probs.append(probs[action])\n",
    "        return action\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        \"\"\"calculate gradient vector of log-probas\"\"\"\n",
    "        y = x @ self.theta\n",
    "        grad_log_p0 = x - x * self.logistic(y)\n",
    "        grad_log_p1 = - x * self.logistic(y)\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        \"\"\"calculate discounted rewards\"\"\"\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.discount_rate + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"update thetas based on gradients, discounted rewards, learning rate\"\"\"\n",
    "        # calculate gradients for each action you actually took\n",
    "        # how much to adjust theta to increase prob of that action\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action]\n",
    "                               for ob, action in zip(self.state_history,\n",
    "                                                     self.action_history)])\n",
    "\n",
    "        # calculate discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(self.reward_history)\n",
    "        # standardize\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        # gradients times discounted rewards\n",
    "        # average gradient over all obs weighted by reward\n",
    "        # how much to update theta to increase reward\n",
    "        update_target = grad_log_p.T @ discounted_rewards\n",
    "\n",
    "        # update theta\n",
    "        self.theta += self.learning_rate*update_target\n",
    "\n",
    "    def save_agent(self):\n",
    "        \"\"\"save agent: pickle self\"\"\"\n",
    "        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n",
    "        pickle.dump(self, open(\"%s.p\" % fullname, \"wb\"))\n",
    "\n",
    "    def load_agent(filename):\n",
    "        \"\"\"load saved agent\"\"\"\n",
    "        new = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n",
    "        print(\"loaded %d results\" % (len(new.results)))\n",
    "        return new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = CARTPOLE_EPISODES\n",
    "RENDER = False\n",
    "\n",
    "def run_experiment(env, theta, learning_rate, discount_rate, AgentClass,\n",
    "                   N_EPISODES=1000, seed=None, comet_enabled=COMET_ENABLED):\n",
    "\n",
    "    if comet_enabled:\n",
    "        experiment = Experiment(project_name=\"DV_Cartpole_Logistic_PG\",\n",
    "                                auto_param_logging=False)\n",
    "        \n",
    "        experiment.log_parameters({\n",
    "            \"discount_rate\": DISCOUNT_RATE,\n",
    "            \"learning_rate\": learning_rate,\n",
    "        })\n",
    "\n",
    "    # initialize environment and policy\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    episode_rewards = []\n",
    "    agent = AgentClass(theta, learning_rate, discount_rate)\n",
    "\n",
    "    # train for N_EPISODES\n",
    "    print(\"Start training: %s\" % time.strftime(\"%H:%M:%S\"))    \n",
    "    for e in range(N_EPISODES):\n",
    "\n",
    "        # run an episode\n",
    "        agent.run_episode(env, render=RENDER)\n",
    "        agent.score_episode(e, N_EPISODES)\n",
    "        if comet_enabled:\n",
    "            experiment.log_metrics({\n",
    "                'episode': len(agent.results),\n",
    "                'reward': agent.total_reward,\n",
    "                'avg_reward': agent.avgscore\n",
    "            })\n",
    "\n",
    "        if e and (e+1) % agent.save_interval == 0:\n",
    "            agent.save_agent()\n",
    "    print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    if comet_enabled:\n",
    "        experiment.end()\n",
    "\n",
    "    return episode_rewards, agent\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "episode_rewards, agent = run_experiment(env,\n",
    "                                        theta=np.random.randn(4)/100,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_rate=0.975,\n",
    "                                        AgentClass=LogisticAgent,\n",
    "                                        N_EPISODES=N_EPISODES,\n",
    "                                        seed=GLOBAL_SEED,\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training progress\n",
    "\n",
    "agent.rlplot('Cartpole Logistic Policy Gradient Training Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view trained agent in action\n",
    "# early stopping is our friend, if last model not best\n",
    "env = gym.make('CartPole-v1')\n",
    "# agent = LogisticAgent.load_agent(OUTPUT_DIR + 'logistic01950')\n",
    "agent.view(render=RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras REINFORCE policy gradient method\n",
    "# Same policy gradient algorithm\n",
    "# Use Keras to define policy network\n",
    "# Allows use of neural network with multiple hidden layers\n",
    "# Keras does the backprop, computes gradients \n",
    "# Also our logistic is binary, only 2 actions\n",
    "# Keras softmax generalizes to n actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_Agent(Agent):\n",
    "    \"\"\"REINFORCE policy gradient method using deep Keras NN\"\"\"\n",
    "    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SIZE, learning_rate=0.0005,\n",
    "                 discount_rate=DISCOUNT_RATE, n_hidden_layers=N_HIDDEN_LAYERS, hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                 activation='relu', reg_penalty=0, dropout=0, filename=\"kreinforce\",\n",
    "                 verbose=True):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_space = list(range(action_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation = activation\n",
    "        self.reg_penalty = reg_penalty\n",
    "        self.dropout = dropout\n",
    "        self.verbose = verbose\n",
    "        self.filename = filename\n",
    "\n",
    "        self.max_score = 0\n",
    "        self.max_avg = 0\n",
    "\n",
    "        self.policy_model = PolicyModel(state_size, action_size,\n",
    "                                        n_hidden_layers=n_hidden_layers,\n",
    "                                        hidden_layer_size=hidden_layer_size)\n",
    "        self.optimizer = keras.optimizers.Adam()        \n",
    "        \n",
    "        self.results = []\n",
    "        self.save_interval = 10\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"reset agent for start of episode\"\"\"\n",
    "        self.timestep = 0\n",
    "        # truncate memory\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def act(self, state, argmax=False):\n",
    "        \"\"\"pick an action using policy_model\"\"\"\n",
    "        logits = self.policy_model.predict(state)\n",
    "        probabilities = tf.nn.softmax(logits)[0]\n",
    "\n",
    "        if argmax:\n",
    "            # for replay choose most likely action\n",
    "            action = np.argmax(probabilities)\n",
    "        else:\n",
    "            # for training sample from actions\n",
    "            action = np.random.choice(self.action_space, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def remember(self):\n",
    "        \"\"\"at each timestep save state, action, reward for future training\"\"\"\n",
    "        self.state_memory.append(self.state)\n",
    "        self.action_memory.append(self.action)\n",
    "        self.reward_memory.append(self.reward)\n",
    "        \n",
    "    def train_step(self, state_memory, actions, deltas):\n",
    "        # compute gradient and update\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_logits = self.policy_model(state_memory)\n",
    "            pred_probs = tf.nn.softmax(pred_logits)\n",
    "            log_probs = tf.math.log(pred_probs)\n",
    "            # mask / squeeze log_probs for only actions we actually took\n",
    "            log_probs = tf.reduce_sum(tf.math.multiply(log_probs, actions), axis=1)\n",
    "            # multiply by discounted_rewards and calculate sum. \n",
    "            # negate because apply does gradient descent(minimizes), we want ascent(maximize)\n",
    "            target = tf.reduce_mean(tf.math.multiply(log_probs, -deltas))\n",
    "            # get gradients of prob x discounted_rewards\n",
    "            gradients = tape.gradient(target, self.policy_model.trainable_variables)\n",
    "            # apply gradient\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.policy_model.trainable_variables))\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\"train the model on experience stored by remember\"\"\"\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        # one-hot actions\n",
    "        actions = np.zeros([len(action_memory), self.action_size])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        disc_rewards = np.zeros_like(reward_memory)\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(len(reward_memory))):\n",
    "            cumulative_rewards = cumulative_rewards * self.discount_rate + reward_memory[i]\n",
    "            disc_rewards[i] = cumulative_rewards\n",
    "            \n",
    "        # standardize\n",
    "        disc_rewards -= np.mean(disc_rewards)\n",
    "        disc_rewards /= np.std(disc_rewards) if np.std(disc_rewards) > 0 else 1\n",
    "      \n",
    "        self.train_step(tf.convert_to_tensor(state_memory), \n",
    "                        tf.convert_to_tensor(actions), tf.convert_to_tensor(disc_rewards))\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {'state_size': self.state_size,\n",
    "                'action_size': self.action_size,\n",
    "                'n_hidden_layers': self.n_hidden_layers,\n",
    "                'hidden_layer_size': self.hidden_layer_size,\n",
    "                'activation': self.activation,\n",
    "                'reg_penalty': self.reg_penalty,\n",
    "                'dropout': self.dropout,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'filename': self.filename,\n",
    "                'verbose': self.verbose,\n",
    "               }\n",
    "    \n",
    "    def save_agent(self):\n",
    "        \"\"\"save agent: pickle self and use Keras native save model\"\"\"\n",
    "        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n",
    "        agent_attrs = self.get_config()\n",
    "        model_attrs = self.policy_model.get_config()\n",
    "        self.policy_model.save_weights(\"%s.h5\" % fullname)\n",
    "        pickle.dump({'agent_attrs': agent_attrs, 'model_attrs': model_attrs}, open(\"%s.p\" % fullname, \"wb\"))\n",
    "\n",
    "    def load_agent(filename):\n",
    "        \"\"\"load saved agent\"\"\"\n",
    "        pickledict = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n",
    "        new = DQN_Agent(state_size=pickledict['agent_attrs']['state_size'],\n",
    "                        action_size=pickledict['agent_attrs']['action_size'],\n",
    "                        discount_rate=pickledict['agent_attrs']['discount_rate'],\n",
    "                        learning_rate=pickledict['agent_attrs']['learning_rate'],\n",
    "                        epsilon=pickledict['agent_attrs']['epsilon'],\n",
    "                        epsilon_decay=pickledict['agent_attrs']['epsilon_decay'],\n",
    "                        epsilon_min=pickledict['agent_attrs']['epsilon_min']                        \n",
    "                       )\n",
    "        for name, value in pickledict['agent_attrs'].items():\n",
    "            setattr(new, name, value)\n",
    "        # make a prediction to fix input size\n",
    "        new.policy_model.predict(np.random.uniform(size=(1, new.state_size))/10)\n",
    "            \n",
    "        new.policy_model.load_weights(\"%s.h5\" % filename, by_name=False)           \n",
    "        for name, value in pickledict['model_attrs'].items():\n",
    "            setattr(new.policy_model, name, value)\n",
    "        print(\"loaded %d results, %d rows of memory, epsilon %.4f\" % (len(new.results),\n",
    "                                                                      len(new.memory),\n",
    "                                                                      new.epsilon))\n",
    "        return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run faster without rendering\n",
    "RENDER = False\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment = Experiment(project_name=\"DV_Cartpole_REINFORCE\",\n",
    "                            auto_param_logging=False)    \n",
    "\n",
    "# https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "if COMET_ENABLED:\n",
    "    experiment.log_parameters({\n",
    "        \"discount_rate\": DISCOUNT_RATE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "    })\n",
    "\n",
    "agent = REINFORCE_Agent(state_size=env.observation_space.shape[0],\n",
    "                        action_size=env.action_space.n,\n",
    "                        n_hidden_layers=0,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        discount_rate=DISCOUNT_RATE,)\n",
    "\n",
    "# train\n",
    "\n",
    "print(\"Start training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "for e in range(N_EPISODES):\n",
    "    agent.run_episode(env)\n",
    "    agent.score_episode(e, N_EPISODES)\n",
    "    if COMET_ENABLED:\n",
    "        experiment.log_metrics({\n",
    "            'episode': len(agent.results),\n",
    "            'reward': agent.total_reward,\n",
    "            'avg_reward': agent.avgscore\n",
    "        })\n",
    "\n",
    "    if e and (e+1) % agent.save_interval == 0:\n",
    "        agent.save_agent()\n",
    "        \n",
    "print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment.end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training progress\n",
    "agent.rlplot(title='Cartpole REINFORCE Agent Training Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view agent in action\n",
    "# agent.predict_model = load_model(OUTPUT_DIR + \"kreinforce01760_predict.h5\")\n",
    "\n",
    "agent.view(render=RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for science let's try on LunarLander\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment = Experiment(project_name=\"DV_LunarLander_REINFORCE\",\n",
    "                            auto_param_logging=False)    \n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "N_EPISODES = LUNARLANDER_EPISODES\n",
    "N_HIDDEN_LAYERS = 2\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "DISCOUNT_RATE = 0.0005\n",
    "LEARNING_RATE = 0.99\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment.log_parameters({\n",
    "        \"n_hidden_layers\": N_HIDDEN_LAYERS,\n",
    "        \"hidden_layer_size\": HIDDEN_LAYER_SIZE,\n",
    "        \"discount_rate\": DISCOUNT_RATE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "    })\n",
    "\n",
    "agent = REINFORCE_Agent(state_size=env.observation_space.shape[0],\n",
    "                        n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "                        hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                        action_size=env.action_space.n,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        discount_rate=DISCOUNT_RATE,)\n",
    "\n",
    "print(\"Start training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "for e in range(N_EPISODES):\n",
    "    agent.run_episode(env)\n",
    "    agent.score_episode(e, N_EPISODES)\n",
    "    if COMET_ENABLED:\n",
    "        experiment.log_metrics({\n",
    "            'episode': len(agent.results),\n",
    "            'reward': agent.total_reward,\n",
    "            'avg_reward': agent.avgscore\n",
    "        })\n",
    "    \n",
    "print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rlplot(title='Lunar Lander Deep Policy Gradient Agent Training Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.view(render=RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with REINFORCE above, we run out each episode trajectory, then update thetas \n",
    "# to get more good/fewer bad outcomes relative to average\n",
    "# for instance suppose we are training Cartpole, getting scores around 200\n",
    "# you run out 200 timesteps and fall over\n",
    "# you want more of the actions at the beginning, fewer of the actions at the end that made it fall over\n",
    "# fine as long as you gradually improve but suppose you have an episode that scores only 10\n",
    "# you don't really want more of any of those actions\n",
    "# maybe we can do better than measuring scores relative to episode average\n",
    "# we build a 'baseline' NN state-value function estimator \n",
    "# after each episode, train the state-value function\n",
    "# in case of cart-pole, function should learn that high deflections, high speed toward edge = bad\n",
    "# train policy agent to use state augmented by state-value deltas instead of deviations from mean\n",
    "# this is 'REINFORCE with baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueModel(Model):\n",
    "    \"\"\"model takes state_size inputs, returns a single state value  \n",
    "    same setup as PolicyModel except last layer is size 1 instead of action_size\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "                 hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                 activation='relu',\n",
    "                 reg_penalty=0.001,\n",
    "                 dropout=0.0625,\n",
    "                 verbose=True\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation = activation\n",
    "        self.reg_penalty = reg_penalty\n",
    "        self.dropout = dropout\n",
    "        self.verbose = verbose\n",
    "                \n",
    "        self.rllayers = []\n",
    "\n",
    "        for i in range(n_hidden_layers):\n",
    "            if verbose:\n",
    "                formatstr = \"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\"\n",
    "                print(formatstr % (i + 1, hidden_layer_size, activation, reg_penalty, dropout))\n",
    "                \n",
    "            # add dropout, only between hidden layers\n",
    "            if i and dropout:\n",
    "                self.rllayers.append(Dropout(dropout, name=\"Dropout%02d\" % i))\n",
    "                \n",
    "            self.rllayers.append(Dense(hidden_layer_size, activation=activation, name=\"Dense%02d\" % i))\n",
    "\n",
    "        self.rllayers.append(Dense(1, activation='linear', name=\"Output\"))\n",
    "\n",
    "    def call(self, x):\n",
    "        # Forward pass\n",
    "        for layer in self.rllayers:\n",
    "            x = layer(x)\n",
    "        return x  \n",
    "\n",
    "    def get_config(self):\n",
    "        return {'state_size': self.state_size,\n",
    "                'action_size': self.action_size,\n",
    "                'n_hidden_layers': self.n_hidden_layers,\n",
    "                'hidden_layer_size': self.hidden_layer_size,\n",
    "                'activation': self.activation,\n",
    "                'reg_penalty': self.reg_penalty,\n",
    "                'dropout': self.dropout,\n",
    "                'verbose': self.verbose,\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceBaseline(REINFORCE_Agent):\n",
    "    \"\"\"REINFORCE with baseline\n",
    "    inherit from REINFORCE_Agent, add action-value model, reimplement train to use value model\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SIZE, learning_rate=0.0005,\n",
    "                 discount_rate=DISCOUNT_RATE, \n",
    "                 n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "                 hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                 activation='relu', reg_penalty=0, dropout=0, filename=\"krb\",):\n",
    "        super().__init__(state_size=state_size, \n",
    "                         action_size=action_size, \n",
    "                         learning_rate=learning_rate,\n",
    "                         discount_rate=discount_rate, \n",
    "                         n_hidden_layers=n_hidden_layers, \n",
    "                         hidden_layer_size=hidden_layer_size,\n",
    "                         activation=activation, \n",
    "                         reg_penalty=reg_penalty, \n",
    "                         dropout=dropout, \n",
    "                         filename=filename)\n",
    "        self.baseline = ValueModel(state_size, action_size, \n",
    "                                   n_hidden_layers=n_hidden_layers, hidden_layer_size=hidden_layer_size)\n",
    "        self.baseline_optimizer = Adam()\n",
    "        self.loss_function = keras.losses.MSE\n",
    "\n",
    "    def train_baseline_step(self, X, Y):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X, Y)).batch(BATCH_SIZE)\n",
    "        for X_batch, Y_batch in train_ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.baseline(X_batch)\n",
    "                loss = self.loss_function(Y_batch, predictions)\n",
    "            gradients = tape.gradient(loss, self.baseline.trainable_variables)\n",
    "            self.baseline_optimizer.apply_gradients(zip(gradients, self.baseline.trainable_variables))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"train action-value model, policy model\"\"\"\n",
    "        \n",
    "        # convert to numpy ndarrays\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "\n",
    "        # one-hot actions\n",
    "        actions = np.zeros([len(action_memory), self.action_size])\n",
    "        actions[np.arange(len(action_memory)), action_memory] = 1\n",
    "\n",
    "        # compute discounted rewards\n",
    "        disc_rewards = np.zeros_like(reward_memory)\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(len(reward_memory))):\n",
    "            cumulative_rewards = cumulative_rewards * self.discount_rate + reward_memory[i]\n",
    "            disc_rewards[i] = cumulative_rewards\n",
    "\n",
    "        # instead of standardizing, compute difference vs. value model\n",
    "        # can be viewed as 'surprise', how much better/worse outcome was than expected\n",
    "        state_values = self.baseline.predict(state_memory).reshape(- 1)\n",
    "        deltas = disc_rewards - state_values\n",
    "        self.train_step(state_memory, actions, deltas)\n",
    "        # train value function against observed rewards\n",
    "        self.train_baseline_step(state_memory, disc_rewards)\n",
    "\n",
    "    def save_agent(self):\n",
    "        \"\"\"save agent: pickle self and use Keras native save model\"\"\"\n",
    "        fullname = \"%s%s%05d\" % (OUTPUT_DIR, self.filename, len(self.results))\n",
    "        self.policy_model.save_weights(\"%s_predict.h5\" % fullname)\n",
    "        self.baseline.save_weights(\"%s_baseline.h5\" % fullname)\n",
    "        pickledict = {'agent_attrs': self.get_config(), \n",
    "                      'policy_model_attrs': self.policy_model.get_config(),\n",
    "                      'baseline_model_attrs': self.baseline.get_config(),\n",
    "                     }\n",
    "        pickle.dump(pickledict, open(\"%s.p\" % fullname, \"wb\"))\n",
    "\n",
    "    def load_agent(filename):\n",
    "        \"\"\"load saved agent\"\"\"\n",
    "        pickledict = pickle.load(open(\"%s.p\" % filename, \"rb\"))\n",
    "        new = DQN_Agent(state_size=pickledict['agent_attrs']['state_size'],\n",
    "                        action_size=pickledict['agent_attrs']['action_size'],\n",
    "                        discount_rate=pickledict['agent_attrs']['discount_rate'],\n",
    "                        learning_rate=pickledict['agent_attrs']['learning_rate'],\n",
    "                        epsilon=pickledict['agent_attrs']['epsilon'],\n",
    "                        epsilon_decay=pickledict['agent_attrs']['epsilon_decay'],\n",
    "                        epsilon_min=pickledict['agent_attrs']['epsilon_min']                        \n",
    "                       )\n",
    "        for name, value in pickledict['agent_attrs'].items():\n",
    "            setattr(new, name, value)\n",
    "        # make a prediction to fix input size\n",
    "        new.policy_model.predict(np.random.uniform(size=(1, new.state_size))/10)\n",
    "        new.policy_model.load_weights(\"%s_predict.h5\" % filename, by_name=False)           \n",
    "        for name, value in pickledict['policy_model_attrs'].items():\n",
    "            setattr(new.policy_model, name, value)\n",
    "        # make a prediction to fix input size\n",
    "        new.baseline.predict(np.random.uniform(size=(1, new.state_size))/10)\n",
    "        new.baseline.load_weights(\"%s_baseline.h5\" % filename, by_name=False)           \n",
    "        for name, value in pickledict['baseline_model_attrs'].items():\n",
    "            setattr(new.baseline, name, value)\n",
    "        print(\"loaded\")\n",
    "        return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run faster without rendering\n",
    "RENDER = False\n",
    "# https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment = Experiment(project_name=\"DV_Cartpole_REINFORCE_Baseline\",\n",
    "                            auto_param_logging=False)    \n",
    "\n",
    "N_EPISODES = CARTPOLE_EPISODES\n",
    "N_HIDDEN_LAYERS = 1\n",
    "HIDDEN_LAYER_SIZE = 32\n",
    "DISCOUNT_RATE = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment.log_parameters({\n",
    "        \"n_hidden_layers\": N_HIDDEN_LAYERS,\n",
    "        \"hidden_layer_size\": HIDDEN_LAYER_SIZE,\n",
    "        \"discount_rate\": DISCOUNT_RATE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "    })\n",
    "\n",
    "agent = ReinforceBaseline(state_size=env.observation_space.shape[0],\n",
    "                          action_size=env.action_space.n,\n",
    "                          learning_rate=LEARNING_RATE,\n",
    "                          discount_rate=DISCOUNT_RATE,\n",
    "                          n_hidden_layers=N_HIDDEN_LAYERS,\n",
    "                          hidden_layer_size=HIDDEN_LAYER_SIZE,\n",
    "                         )\n",
    "\n",
    "# train\n",
    "N_EPISODES = CARTPOLE_EPISODES\n",
    "print(\"Start training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "for e in range(N_EPISODES):\n",
    "    agent.run_episode(env)\n",
    "    agent.score_episode(e, N_EPISODES)\n",
    "    if COMET_ENABLED:\n",
    "        experiment.log_metrics({\n",
    "            'episode': len(agent.results),\n",
    "            'reward': agent.total_reward,\n",
    "            'avg_reward': agent.avgscore\n",
    "        })   \n",
    "    if e and (e+1) % agent.save_interval == 0:\n",
    "        agent.save_agent()\n",
    "print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "if COMET_ENABLED:\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rlplot(title='Cartpole REINFORCE w/Baseline Agent Training Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.view(render=RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try on LunarLander\n",
    "N_EPISODES = LUNARLANDER_EPISODES\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "agent = ReinforceBaseline(state_size=env.observation_space.shape[0],\n",
    "                          action_size=env.action_space.n,\n",
    "                          n_hidden_layers=2,\n",
    "                          hidden_layer_size=64,\n",
    "                          learning_rate=0.0005,\n",
    "                          discount_rate=0.99, \n",
    "                          filename='llrb')\n",
    "\n",
    "print(\"Start training: %s\" % time.strftime(\"%H:%M:%S\"))\n",
    "for e in range(N_EPISODES):\n",
    "    agent.run_episode(env)\n",
    "    agent.score_episode(e, N_EPISODES)\n",
    "    if e and (e+1) % agent.save_interval == 0:\n",
    "        agent.save_agent()\n",
    "print(\"\\nFinish training: %s\" % time.strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rlplot(\"Lunar Lander REINFORCE with Baseline Training Progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.view(render=RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run faster without rendering\n",
    "RENDER = False\n",
    "# https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(GLOBAL_SEED)\n",
    "\n",
    "agent = ReinforceBaseline(state_size=env.observation_space.shape[0],\n",
    "                          action_size=env.action_space.n,\n",
    "                          n_hidden_layers=2,\n",
    "                          hidden_layer_size=64,\n",
    "                          learning_rate=0.0005,\n",
    "                          discount_rate=0.98,)\n",
    "agent.predict_model = load_model(\"llrb_good_predict.h5\")\n",
    "agent.baseline = load_model(\"llrb_good_V.h5\")\n",
    "agent.view(render=RENDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use RLlib - state of the art library, instead of rolling our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "n_cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "n_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True, log_to_driver=False, webui_host='0.0.0.0')\n",
    "# https://ray.readthedocs.io/en/latest/package-ref.html#ray.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/ppo.py\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "\n",
    "ppo_config = DEFAULT_CONFIG.copy()\n",
    "if n_gpus:\n",
    "    ppo_config['num_gpus'] = n_gpus\n",
    "    ppo_config['tf_session_args']['device_count']['GPU'] = n_gpus\n",
    "\n",
    "ppo_config['num_workers'] = 1\n",
    "ppo_config['num_sgd_iter'] = 2\n",
    "ppo_config['sgd_minibatch_size'] = 128\n",
    "ppo_config['lr'] = 0.0003\n",
    "ppo_config['gamma'] = 0.99\n",
    "ppo_config['model']['fcnet_hiddens'] = [64, 64]\n",
    "ppo_config['timesteps_per_iteration'] = 2000\n",
    "ppo_config['train_batch_size'] = 8000\n",
    "ppo_config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "agent = PPOTrainer(ppo_config, env_name)\n",
    "result = agent.train()\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ray-project/ray/blob/master/python/ray/tune/tune.py\n",
    "from ray import tune\n",
    "ray.init(ignore_reinit_error=True)\n",
    "env_name = 'CartPole-v1'\n",
    "ppo_config = {\n",
    "    \"env\": env_name,\n",
    "    \"num_workers\": 1,\n",
    "    'model': {\n",
    "        'fcnet_hiddens': tune.grid_search([\n",
    "                                           [16, 16], [32, 32], [64, 64], [128, 128],\n",
    "                                          ])\n",
    "    },        \n",
    "    'train_batch_size': 1000,\n",
    "    \"lr\": tune.grid_search([0.0003, 0.0001]),\n",
    "    'gamma': tune.grid_search([0.99, 0.999]),\n",
    "    \"eager\": False,\n",
    "    'num_gpus': n_gpus  \n",
    "}\n",
    "                      \n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    name='cartpole_test',\n",
    "    verbose=1,\n",
    "\n",
    "    stop={\"episode_reward_mean\": 300},  # stop when a parameter set is able to reach 300 timesteps\n",
    "    config = ppo_config,\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_score_attr='episode_reward_mean',\n",
    "    num_samples=1,  # for grid search, number of times to run each hyperparameter combo\n",
    "    #     with_server=True,\n",
    "    #     server_port=8267,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = analysis.trial_dataframes\n",
    "\n",
    "# Plot by epoch\n",
    "ax = None  # This plots everything on the same plot\n",
    "for d in dfs.values():\n",
    "    ax = d.episode_reward_mean.plot(ax=ax, legend=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe().sort_values(['timesteps_total','episode_reward_mean'])[['config/lr', \n",
    "                                                                             'config/gamma', \n",
    "                                                                             'config/model', \n",
    "                                                                             'episode_reward_mean', \n",
    "                                                                             'timesteps_total']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[['config/lr', 'timesteps_total']].groupby('config/lr').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[['config/gamma', 'timesteps_total']].groupby('config/gamma').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aframe = analysis.dataframe()\n",
    "aframe['config/model'] = aframe['config/model'].astype(str)\n",
    "aframe[['config/model', 'timesteps_total']] \\\n",
    "    .groupby(['config/model']) \\\n",
    "    .mean() \\\n",
    "    .sort_values('timesteps_total') \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zframe = analysis.dataframe()[['config/lr', 'config/gamma', 'timesteps_total']]\n",
    "matrix = pd.pivot_table(zframe, \n",
    "                        values='timesteps_total', \n",
    "                        index=['config/lr'], \n",
    "                        columns=['config/gamma'],\n",
    "                        aggfunc=np.mean)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(matrix, annot=True, fmt='.0f')\n",
    "plt.ylabel('lr')\n",
    "plt.xlabel('gamma')\n",
    "plt.title(\"Hyperparameter matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = analysis.get_best_logdir(\"timesteps_total\", mode=\"min\")\n",
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/params.json' % logdir) as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=n_cpus, num_gpus=n_gpus, ignore_reinit_error=True, log_to_driver=False, webui_host='0.0.0.0')\n",
    "\n",
    "ppo_config = {\n",
    "    \"env\": env_name,\n",
    "    \"num_workers\": n_cpus - 1,\n",
    "    'model': {\n",
    "        'fcnet_hiddens': [32, 32]\n",
    "    },        \n",
    "    'train_batch_size': 10000,\n",
    "    \"lr\": 0.0003,\n",
    "    'gamma': 0.99,\n",
    "    \"eager\": False,\n",
    "    'num_gpus': n_gpus  \n",
    "}\n",
    "                      \n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    name='cartpole_test',\n",
    "    verbose=1,\n",
    "\n",
    "    stop={\"episode_reward_mean\": 500},  # stop when a parameter set is able to reach 500 timesteps\n",
    "    config = ppo_config,\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_score_attr='episode_reward_mean',\n",
    "    num_samples=1,  # for grid search, number of times to run each hyperparameter combo\n",
    "    #     with_server=True,\n",
    "    #     server_port=8267,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(analysis.trial_dataframes.keys())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/ubuntu/ray_results/cartpole_test/PPO_CartPole-v1_1c7d6f00_2020-02-13_02-25-33iw9gmucd/checkpoint_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint and do the runout\n",
    "ckpoint = '/home/ubuntu/ray_results/cartpole_test/PPO_CartPole-v1_1c7d6f00_2020-02-13_02-25-33iw9gmucd/checkpoint_18/checkpoint-18'\n",
    "trainer = PPOTrainer(config=ppo_config, env=env_name)\n",
    "trainer.restore(ckpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "#env.seed(GLOBAL_SEED)\n",
    "\n",
    "state = env.reset()\n",
    "trainer.compute_action(state)\n",
    "\n",
    "done = False\n",
    "\n",
    "# run an episode\n",
    "timestep = 0\n",
    "r = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    r += reward\n",
    "    timestep += 1\n",
    "print(r)\n",
    "env.close()\n",
    "timestep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
