{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, deque\n",
    "from itertools import product, chain\n",
    "import copy\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 3\n",
    "MAX_MOVES = BOARD_SIZE * BOARD_SIZE\n",
    "INIT_BOARD = ((' ',' ',' '),(' ',' ',' '),(' ',' ',' '),)\n",
    "\n",
    "def print_board(board):\n",
    "    \"\"\"return string representation of board\"\"\"\n",
    "    retval = ''\n",
    "    for i, row in enumerate(board):\n",
    "        if i:\n",
    "            retval += \"===========\\n\"\n",
    "        retval += \" %s\\n\" % \" | \".join(row)\n",
    "    retval += \"\\n\"\n",
    "    return retval\n",
    "\n",
    "class Game:\n",
    "    \"\"\"Maintain game state\"\"\"\n",
    "    \n",
    "    def __init__(self, startplayer='X'):\n",
    "        self.board = INIT_BOARD\n",
    "        self.player = startplayer\n",
    "        self.history = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"string representation of board\"\"\"\n",
    "        return print_board(self.board)\n",
    "        \n",
    "    def play(self, move, record=True):\n",
    "        \"\"\"given move as row, col, player tuple, update board\n",
    "        record=True: update game state and return board; \n",
    "        record=False: just return resulting board, i.e. for evaluation\n",
    "        \"\"\"\n",
    "        i, j, player = move\n",
    "        if player != self.player:\n",
    "            raise(Exception(\"play: wrong player %s\" % (player)))\n",
    "        elif i >= len(self.board) or j >= len(self.board):\n",
    "            raise(Exception(\"play: bad square coords %d, %d\" % (i,j)))\n",
    "        elif self.board[i][j] != ' ':\n",
    "            raise(Exception(\"play: move to non-empty square\"))\n",
    "        else:\n",
    "            # new tuple, same except set square = player\n",
    "            new_board = tuple(row if r != i \\\n",
    "                              else tuple(player if c==j else square for c, square in enumerate(row)) \\\n",
    "                              for r, row in enumerate(self.board))\n",
    "            if record:\n",
    "                self.board=new_board\n",
    "                self.history.append(self.board)\n",
    "                self.player = 'O' if self.player=='X' else 'X'\n",
    "                \n",
    "        return new_board\n",
    "    \n",
    "    def is_winner(self, player='X'):\n",
    "        b_a = np.array(self.board)\n",
    "        if any(all(b_a[i, :] == player) for i in range(BOARD_SIZE)):\n",
    "            return True\n",
    "        if any(all(b_a[:, j] == player) for j in range(BOARD_SIZE)):\n",
    "            return True\n",
    "        if all(np.diagonal(b_a) == player):\n",
    "            return True\n",
    "        if all(np.diagonal(np.fliplr(b_a)) == player):\n",
    "            return True\n",
    "        # no winning conditions are True\n",
    "        return False\n",
    "    \n",
    "g = Game()\n",
    "g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.play((1,1,'X'))\n",
    "# some bad moves\n",
    "#g.play((2,2,'X')) # wrong player\n",
    "#g.play((1,1,'O')) # occupied square\n",
    "#g.play((3,1,'O')) # off board\n",
    "g.play((0,1,'O'))\n",
    "g.play((0,0,'X'))\n",
    "g.play((2,2,'O'))\n",
    "g.play((2,0,'X'))\n",
    "g.play((1,0,'O'))\n",
    "g.play((0,2,'X'))\n",
    "print(g.is_winner('O'))\n",
    "print(g.is_winner('X'))\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check winning boards\n",
    "for bx in [((' ', ' ', ' '),(' ', ' ', ' '),(' ', ' ', ' ')),\n",
    "           (('X', 'X', 'X'),(' ', ' ', ' '),(' ', ' ', ' ')),\n",
    "           ((' ', ' ', ' '),('X', 'X', 'X'),(' ', ' ', ' ')),\n",
    "           ((' ', ' ', ' '),(' ', ' ', ' '),('X', 'X', 'X')),\n",
    "           (('X', ' ', ' '),('X', ' ', ' '),('X', ' ', ' ')),\n",
    "           ((' ', 'X', ' '),(' ', 'X', ' '),(' ', 'X', ' ')),\n",
    "           ((' ', ' ', 'X'),(' ', ' ', 'X'),(' ', ' ', 'X')),\n",
    "           (('X', ' ', ' '),(' ', 'X', ' '),(' ', ' ', 'X')),\n",
    "           ((' ', ' ', 'X'),(' ', 'X', ' '),('X', ' ', ' ')),]:\n",
    "    g.board = bx\n",
    "    print(g.is_winner('X'))\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.25\n",
    "DISCOUNT_RATE = 0.05\n",
    "EXPLORATION_RATE = 0.025 \n",
    "\n",
    "class RLagent:\n",
    "    \"\"\"Simple reinforcement learning agent\n",
    "    initialize with an array (defaultdict) with value for each board\n",
    "    select_move: given a board, determine valid moves, play move with best value \n",
    "    (or explore random move with probability EXPLORATION_RATE)\n",
    "    train: traverse all game boards in game state history, \n",
    "    update value array based on winner, discount rate, learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 game, \n",
    "                 V_dict,\n",
    "                 player='O',\n",
    "                 learning_rate=LEARNING_RATE,\n",
    "                 discount_rate=DISCOUNT_RATE,\n",
    "                 exploration_rate=EXPLORATION_RATE\n",
    "                ):\n",
    "        self.game = game\n",
    "        self.V = V_dict\n",
    "        self.player = player\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        \n",
    "    def valid_moves(self):\n",
    "        retlist = []\n",
    "        for i, row in enumerate(self.game.board):\n",
    "            for j, colval in enumerate(row):\n",
    "                if colval == ' ':\n",
    "                    move = (i,j, self.player)\n",
    "                    retlist.append(move)\n",
    "        return retlist\n",
    "\n",
    "    def select_move(self, verbose=False, exploration_rate=None):\n",
    "        \"\"\"select best scoring action, \n",
    "        if more than one have best score pick random from best\"\"\"\n",
    "        moves = self.valid_moves()\n",
    "\n",
    "        if not exploration_rate:\n",
    "            exploration_rate = self.exploration_rate\n",
    "        \n",
    "        # choose a random move some % of time specified by exploration rate\n",
    "        if random.uniform(0,1) < exploration_rate:\n",
    "            # set all scores to 0.5\n",
    "            scores = [0.5 for b in moves]\n",
    "            boards = [self.game.play(move, record=False) for move in moves]\n",
    "            if verbose:\n",
    "                print(\"Random exploration\")\n",
    "        else:\n",
    "            # look up boards without recording\n",
    "            boards = [self.game.play(move, record=False) for move in moves]\n",
    "            # look up scores\n",
    "            scores = [self.V[board] for board in boards]\n",
    "\n",
    "        if verbose:\n",
    "            for i, s in enumerate(scores):\n",
    "                print(\"%d.  %.04f\\n%s\" % (i, s, print_board(boards[i])))\n",
    "\n",
    "        # if player is X, choose highest prob of X winning else lowest prob of X winning\n",
    "        best_score = max(scores) if self.player == 'X' else min(scores)\n",
    "        # get all scores matching best\n",
    "        best_moves = [moves[i] for i, score in enumerate(scores) if score == best_score]\n",
    "        # pick one\n",
    "        return random.choice(best_moves)\n",
    "\n",
    "    def train(self):\n",
    "        # update value function based on winner at end of game\n",
    "        \n",
    "        # last board gets value of 1 if X wins, 0 if O wins, 0.5 if draw\n",
    "        reward = 1  if self.game.is_winner('X') \\\n",
    "            else -1 if self.game.is_winner('O') \\\n",
    "            else 0\n",
    "        \n",
    "        for b in reversed(self.game.history):\n",
    "            # update value of each board you see, by (learning rate %) of the way to current reward\n",
    "            old = self.V[b]\n",
    "            self.V[b] = old + (reward - old) * self.learning_rate\n",
    "            # discount reward as boards get older \n",
    "            reward = reward * (1-self.discount_rate)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"old %.04f new %.04f\\n%s\"% (old, self.V[b], print_board(b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanAgent:\n",
    "    \"\"\"human player, get moves from input\"\"\"\n",
    "\n",
    "    def __init__(self, game, player):\n",
    "        self.game = game\n",
    "        self.player = player\n",
    "        \n",
    "    def get_dim(self, prompt):\n",
    "        \"\"\"get a single row or column input\"\"\"\n",
    "        dim = None\n",
    "        while dim not in range(1,BOARD_SIZE+1):\n",
    "            print(prompt)\n",
    "            inputstr = input()\n",
    "            dim = int(inputstr) if inputstr else -1\n",
    "        return int(dim)-1\n",
    "\n",
    "    def get_move(self):\n",
    "        while True:\n",
    "            row = self.get_dim(\"Enter row: \")\n",
    "            col = self.get_dim(\"Enter column: \")\n",
    "            try:\n",
    "                self.game.play((row, col, self.player))\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                continue\n",
    "            break\n",
    "        return row, col\n",
    "\n",
    "def play_again():\n",
    "    print('Play again? (y or n)')\n",
    "    return input().lower().startswith('y')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.4\n",
    "DISCOUNT_RATE = 0.05\n",
    "EXPLORATION_RATE = 0.1\n",
    "QUEUE_LEN = 1000\n",
    "INPUT_DIM=9\n",
    "\n",
    "class DeepRLagent:\n",
    "    \"\"\"Deep learning agent\n",
    "    Instead of updating a V dict in training, add experienced reward values to pandas dataframe\n",
    "    then train neural net to predict boards based on the experienced values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 game, \n",
    "                 player='O',\n",
    "                 learning_rate=LEARNING_RATE,\n",
    "                 discount_rate=DISCOUNT_RATE,\n",
    "                 exploration_rate=EXPLORATION_RATE\n",
    "                ):\n",
    "        self.game = game\n",
    "        self.player = player\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.queue = deque(maxlen=1000)\n",
    "        self.V_model = self.build_ols_model(input_size = INPUT_DIM,\n",
    "                                            n_hidden_layers=3, \n",
    "                                            largest_layer_size=256,\n",
    "                                            activation='tanh',\n",
    "                                            reg_penalty=0.0,\n",
    "                                            dropout=0.0,\n",
    "                                            verbose=False)        \n",
    "        \n",
    "    def build_ols_model(self,\n",
    "                        input_size = INPUT_DIM, \n",
    "                        n_hidden_layers=1, \n",
    "                        largest_layer_size=32, \n",
    "                        activation='relu',\n",
    "                        reg_penalty=0.0,\n",
    "                        dropout=False,\n",
    "                        verbose=True\n",
    "                       ):\n",
    "\n",
    "        model = Sequential()\n",
    "        hidden_layer_size=largest_layer_size\n",
    "\n",
    "        for i in range(n_hidden_layers):\n",
    "            if verbose:\n",
    "                print(\"layer %d size %d, %s, reg_penalty %.8f, dropout %.3f\" % (i + 1, \n",
    "                                                                                hidden_layer_size, \n",
    "                                                                                activation,\n",
    "                                                                                reg_penalty,\n",
    "                                                                                dropout,\n",
    "                                                                               ))\n",
    "            if i and dropout:\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "            if i==0: # first layer, specify input shape\n",
    "                model.add(Dense(input_shape=(input_size,),\n",
    "                                units = hidden_layer_size, \n",
    "                                activation = activation,\n",
    "                                kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                                kernel_regularizer=keras.regularizers.l2(reg_penalty),\n",
    "                                name = \"Dense%02d\" % i))\n",
    "            else: #use implicit input shape\n",
    "                model.add(Dense(units = hidden_layer_size, \n",
    "                                activation = activation,\n",
    "                                kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                                kernel_regularizer=keras.regularizers.l2(reg_penalty),\n",
    "                                name = \"Dense%02d\" % i))\n",
    "\n",
    "            hidden_layer_size = hidden_layer_size // 2\n",
    "\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        if verbose:\n",
    "            print(model.summary())\n",
    "\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def valid_moves(self):\n",
    "        retlist = []\n",
    "        for i, row in enumerate(self.game.board):\n",
    "            for j, colval in enumerate(row):\n",
    "                if colval == ' ':\n",
    "                    move = (i,j, self.player)\n",
    "                    retlist.append(move)\n",
    "        return retlist\n",
    "\n",
    "    def select_move(self, verbose=False, exploration_rate=None):\n",
    "        \"\"\"select best scoring action, \n",
    "        if more than one have best score pick random from best\"\"\"\n",
    "        moves = self.valid_moves()\n",
    "\n",
    "        if not exploration_rate:\n",
    "            exploration_rate = self.exploration_rate\n",
    "        \n",
    "        # choose a random move some % of time specified by exploration rate\n",
    "        if random.uniform(0,1) < exploration_rate:\n",
    "            # set all scores to 0.5\n",
    "            scores = [0.5 for b in moves]\n",
    "            boards = [self.game.play(move, record=False) for move in moves]\n",
    "            if verbose:\n",
    "                print(\"Random exploration\")\n",
    "        else:\n",
    "            # look up boards without recording\n",
    "            boards = [self.game.play(move, record=False) for move in moves]\n",
    "            # look up scores\n",
    "            flatboards = [np.array(self.flatten(b)).reshape(1,-1) for b in boards]\n",
    "            scores = [self.V_model.predict(b) for b in flatboards]\n",
    "\n",
    "        if verbose:\n",
    "            for i, s in enumerate(scores):\n",
    "                print(\"%d.  %.04f\\n%s\" % (i, s, print_board(boards[i])))\n",
    "\n",
    "        # if player is X, choose highest prob of X winning else lowest prob of X winning\n",
    "        best_score = max(scores) if self.player == 'X' else min(scores)\n",
    "        # get all scores matching best\n",
    "        best_moves = [moves[i] for i, score in enumerate(scores) if score == best_score]\n",
    "        # pick one\n",
    "        return random.choice(best_moves)\n",
    "\n",
    "    def flatten(self, b):\n",
    "        \"\"\"convert board to a flat array of ints representation\"\"\"\n",
    "        #flatten\n",
    "        retlist = list(chain.from_iterable(b))\n",
    "        # convert to ints\n",
    "        retlist = [1 if player == 'X' else -1 if player=='O' else 0 for player in retlist]\n",
    "        return retlist\n",
    "    \n",
    "    def train(self):\n",
    "        # update value function based on winner at end of game\n",
    "        \n",
    "        # last board gets value of 1 if X wins, 0 if O wins, 0.5 if draw\n",
    "        reward = 1  if self.game.is_winner('X') \\\n",
    "            else -1 if self.game.is_winner('O') \\\n",
    "            else 0\n",
    "        \n",
    "        for b in reversed(self.game.history):\n",
    "            # append board, reward to queue\n",
    "            # flatten\n",
    "            observation = self.flatten(b)\n",
    "            observation.append(reward)\n",
    "            self.queue.append(observation)\n",
    "            # discount value as boards get older \n",
    "            reward = reward * (1-self.discount_rate)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"new reward %.04f\\n%s\"% (reward, print_board(b)))\n",
    "                \n",
    "            train_df = pd.DataFrame(self.queue)\n",
    "            train_X = train_df.iloc[:,:9]\n",
    "            train_y = train_df.iloc[:,-1]\n",
    "            self.V_model.fit(train_X,\n",
    "                             train_y,\n",
    "                             batch_size=QUEUE_LEN//4,\n",
    "                             epochs=1,\n",
    "                             verbose=0\n",
    "                            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play human v. human\n",
    "\n",
    "while True:\n",
    "    g = Game()    \n",
    "    playerX = HumanAgent(g, 'X')\n",
    "    playerO = HumanAgent(g, 'O')\n",
    "    \n",
    "    max_moves = BOARD_SIZE * BOARD_SIZE\n",
    "    winner = None\n",
    "    \n",
    "    for _ in range(max_moves):\n",
    "        clear_output()\n",
    "        print(g)\n",
    "        \n",
    "        player = g.player\n",
    "        if player == 'X':\n",
    "            row, col = playerX.get_move()\n",
    "        else:\n",
    "            row, col = playerO.get_move()\n",
    "\n",
    "        if g.is_winner(player):\n",
    "            winner = player\n",
    "            break\n",
    "\n",
    "    clear_output()\n",
    "    print(g)\n",
    "            \n",
    "    if winner is None:\n",
    "        print(\"Draw\")\n",
    "    else:\n",
    "        print(\"%s wins!\" % winner)\n",
    "\n",
    "    if not play_again():\n",
    "        print(\"Bye!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# play a bunch of games computer v. computer and update V table\n",
    "\n",
    "START_EXPLORATION_RATE = 0.25\n",
    "NUM_GAMES = 99999\n",
    "V = defaultdict(lambda: 0)\n",
    "verbose = False\n",
    "\n",
    "def play_game(V,\n",
    "              board_size=BOARD_SIZE,\n",
    "              exploration_rate=START_EXPLORATION_RATE,\n",
    "              verbose=verbose):\n",
    "\n",
    "    g = Game()    \n",
    "    playerX = RLagent(g, V, 'X')\n",
    "    playerO = RLagent(g, V, 'O')\n",
    "    \n",
    "    max_moves = BOARD_SIZE * BOARD_SIZE\n",
    "    winner = None\n",
    "\n",
    "    for move_counter in range(max_moves):        \n",
    "        player = g.player\n",
    "\n",
    "        if player == 'X':\n",
    "            move = playerX.select_move(verbose, exploration_rate=exploration_rate)\n",
    "        else:\n",
    "            move = playerO.select_move(verbose, exploration_rate=exploration_rate)\n",
    "        g.play(move)\n",
    "        \n",
    "        if g.is_winner(player):\n",
    "            winner = player\n",
    "            break\n",
    "\n",
    "    if winner is None:\n",
    "        print(\"Draw\")\n",
    "    else:\n",
    "        print(\"%s wins!\" % winner)\n",
    "\n",
    "    if verbose:\n",
    "        for i, b in enumerate(g.history):\n",
    "            print(\"Move %d\" % i)\n",
    "            print(V[b])\n",
    "            print(print_board(b))\n",
    "        \n",
    "    # update V\n",
    "    # players share V array and g game history so we can train either player, only train once\n",
    "    playerO.train()\n",
    "\n",
    "\n",
    "for game_counter in range(NUM_GAMES):\n",
    "    # linear epsilon decay\n",
    "    exploration_rate = (1 - game_counter/NUM_GAMES) * START_EXPLORATION_RATE\n",
    "    play_game(V, exploration_rate=exploration_rate)\n",
    "    if game_counter % 1000 == 999:\n",
    "        print(\"%s: %6d\" % (time.strftime(\"%H:%M:%S\"), game_counter+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out a few V values\n",
    "\n",
    "b = ((' ', ' ', ' '),(' ', ' ', ' '),(' ', ' ', ' '))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = ((' ', ' ', ' '),(' ', ' ', ' '),('X', ' ', ' '))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = ((' ', 'O', ' '),(' ', ' ', ' '),('X', ' ', ' '))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = (('X', 'O', ' '),(' ', ' ', ' '),('X', ' ', ' '))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = (('X', 'O', ' '),('O', ' ', ' '),('X', ' ', ' '))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = (('X', 'O', ' '),('O', ' ', ' '),('X', ' ', 'X'))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = (('X', 'O', ' '),('O', 'O', ' '),('X', ' ', 'X'))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n",
    "b = (('X', 'O', ' '),('O', 'O', ' '),('X', 'X', 'X'))\n",
    "print(\"%f\\n%s\"% (V[b], print_board(b)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play human vs. computer and learn interactively\n",
    "\n",
    "while True:\n",
    "    g = Game()    \n",
    "    playerX = HumanAgent(g, 'X')\n",
    "    playerO = RLagent(g, V, 'O')\n",
    "    \n",
    "    max_moves = BOARD_SIZE * BOARD_SIZE\n",
    "    winner = None\n",
    "    \n",
    "    for _ in range(max_moves):\n",
    "        clear_output()\n",
    "        print(g)\n",
    "        \n",
    "        player = g.player\n",
    "        if player == 'X':\n",
    "            row, col = playerX.get_move()\n",
    "        else:\n",
    "            move = playerO.select_move(verbose=False, exploration_rate=0)            \n",
    "            g.play(move)\n",
    "\n",
    "        if g.is_winner(player):\n",
    "            winner = player\n",
    "            break\n",
    "\n",
    "    clear_output()\n",
    "    print(g)\n",
    "    if winner is None:\n",
    "        print(\"Draw\")\n",
    "    else:\n",
    "        print(\"%s wins!\" % winner)\n",
    "\n",
    "    if not play_again():\n",
    "        print(\"Bye!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use DeepRLAgent\n",
    "# play a bunch of games computer v. computer and update V table\n",
    "\n",
    "# store history in pandas\n",
    "# train incrementally\n",
    "\n",
    "START_EXPLORATION_RATE = 0.1\n",
    "NUM_GAMES = 9999\n",
    "#V = defaultdict(lambda: 0)\n",
    "verbose = False\n",
    "\n",
    "def play_game(V,\n",
    "              board_size=BOARD_SIZE,\n",
    "              exploration_rate=START_EXPLORATION_RATE,\n",
    "              verbose=verbose):\n",
    "\n",
    "    g = Game()    \n",
    "    playerX = DeepRLagent(g, 'X')\n",
    "    playerO = DeepRLagent(g, 'O')\n",
    "    \n",
    "    max_moves = BOARD_SIZE * BOARD_SIZE\n",
    "    winner = None\n",
    "\n",
    "    for move_counter in range(max_moves):        \n",
    "        player = g.player\n",
    "\n",
    "        move = playerX.select_move(verbose, exploration_rate=exploration_rate) if player == 'X' \\\n",
    "            else playerO.select_move(verbose, exploration_rate=exploration_rate)\n",
    "        g.play(move)\n",
    "        \n",
    "        if g.is_winner(player):\n",
    "            winner = player\n",
    "            break\n",
    "\n",
    "    if winner is None:\n",
    "        print(\"Draw\")\n",
    "    else:\n",
    "        print(\"%s wins!\" % winner)\n",
    "\n",
    "    if verbose:\n",
    "        for i, b in enumerate(g.history):\n",
    "            print(\"Move %d\" % i)\n",
    "            print(V[b])\n",
    "            print(print_board(b))\n",
    "        \n",
    "    # update V\n",
    "    playerO.train()\n",
    "\n",
    "for game_counter in range(NUM_GAMES):\n",
    "    # linear epsilon decay\n",
    "    exploration_rate = (1 - game_counter/NUM_GAMES) * START_EXPLORATION_RATE\n",
    "    play_game(V, exploration_rate=exploration_rate)\n",
    "    if game_counter % 1000 == 999:\n",
    "        print(\"%s: %6d\" % (time.strftime(\"%H:%M:%S\"), game_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv\n",
    "z = defaultdict(list)\n",
    "\n",
    "for s, v in V.items():\n",
    "    # flatten\n",
    "    s = tuple(chain.from_iterable(s))\n",
    "    # map s to floats\n",
    "    #s = tuple(0 if i==' ' else 1 if i=='X' else -1 for i in s)\n",
    "    #templist = z[s]\n",
    "    #templist.append(v)\n",
    "    z[s]=v\n",
    "\n",
    "# z2=dict()\n",
    "# for k, l in z.items():\n",
    "#     z2[k]=sum(l)/len(l)\n",
    "\n",
    "Vdf = pd.DataFrame(z.keys())\n",
    "Vdf['val']=z.values()\n",
    "Vdf.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vdf.to_csv('V.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vdf = pd.read_csv('V.csv')\n",
    "Vdf = Vdf[['0', '1', '2', '3', '4', '5', '6', '7', '8', 'val']]\n",
    "z = defaultdict(lambda: 0.5)\n",
    "# need to make tuples to re-use\n",
    "for row in range(len(Vdf)):\n",
    "    b = ((Vdf.iloc[row][0], Vdf.iloc[row][1], Vdf.iloc[row][2]),\n",
    "         (Vdf.iloc[row][3], Vdf.iloc[row][4], Vdf.iloc[row][5]),\n",
    "         (Vdf.iloc[row][6], Vdf.iloc[row][7], Vdf.iloc[row][8]),\n",
    "        )\n",
    "    z[b] = Vdf.iloc[row]['val']\n",
    "\n",
    "Vdf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
