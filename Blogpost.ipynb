{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*once one of my pups found half a roast chicken in the corner of a parking lot and we had to visit that exact same corner every day for about fifty years because for dogs hope springs eternal when it comes to half a roast chicken - [darth](https://twitter.com/darth/status/1057075608063139840)* (possibly embed tweet)\n",
    "\n",
    "*Properly used, positive reinforcement is extremely powerful. - [B. F. Skinner](https://www.brainyquote.com/authors/b-f-skinner-quotes)*\n",
    "\n",
    "Tic-Tac-Toe is a simple game. If both sides play perfectly, neither can win. But if one plays imperfectly, the other can exploit the flaws in the other's strategy. \n",
    "\n",
    "Does that sound a little like trading?\n",
    "\n",
    "In this post, we will explore reinforcement learning, and apply it, first to learn an algorithm to play Tic-Tac-Toe, and then learn to trade a moderately non-random price series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-Tac-Toe With Simple Reinforcement Learning\n",
    "\n",
    "Here's an algorithm that will learn an exploitive Tic-Tac-Toe strategy, and adapt over time if its opponent learns:\n",
    "\n",
    "1) Make a big table of all possible Tic-Tac-Toe boards. \n",
    "\n",
    "2) Initialize the table to assign a value of 0 to each board, 1.0 where X has won, -1.0 where O has won.\n",
    "\n",
    "3) Play with your opponent. At each move, pick the best available move in your table, or if several are tied for best, pick one at random. Occasionally, pick a move at random just to make sure you explore the whole state space, and to keep your opponent on their toes. \n",
    "\n",
    "4) After each game, back up through all the boards that were played. Update the value table as follows:\n",
    "\t- When X wins, update each board's value part of the way to 1. \n",
    "\t- When O wins, update part of the way to -1.  \n",
    "\t- When they tie, update part of the way to 0 \n",
    "\n",
    "This is a profoundly dumb algorithm in the finest sense of the word. It knows almost nothing about the game of Tic-Tac-Toe, but it works. It can't reason about the game. It needs a lot of training. It can't generalize to boards it hasn't seen (footnote: even if they are isomorphic to boards it has seen. When you think about it, there are only 3 starting moves, board center, corner, center side. Flipping or rotating the board shouldn't change the value of a position or how to play it.) . It doesn't learn the globally optimal strategy. \n",
    "\n",
    "But over time, this algorithm learns, it exploits flaws in its opponent's strategy, and if the opponent changes tactics, it adapts.\n",
    "\n",
    "This is *reinforcement learning*. (link to code)\n",
    "\n",
    "More sophisticated reinforcement learning algorithms enable [robots to walk on four or two legs](https://www.youtube.com/watch?v=xXrDnq1RPzQ), [driverless cars to drive](https://www.youtube.com/watch?v=eRwTbRtnT1I), computers to play [Atari](https://deepsense.ai/playing-atari-with-deep-reinforcement-learning-deepsense-ais-approach/) and [poker](https://www.engadget.com/2017/02/10/libratus-ai-poker-winner/?guccounter=1) and [Go](https://deepmind.com/blog/article/alphago-zero-starting-scratch), in some cases better than humans. \n",
    "\n",
    "In the parts that follow, we'll extend the Tic-Tac-Toe example to more complex deep reinforcement learning, and try to build a reinforcement learning trading robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Concepts\n",
    "\n",
    "But first, how does reinforcement learning in general work?\n",
    "\n",
    "![agent-environment](RL1.png \"Figure 1\")\n",
    "\n",
    "All figures are from the [lectures of David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) a leading reinforcement learning researcher known for the [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) project, among others.\n",
    "\n",
    "1) At time *t*, The *agent* observes the environment *state* *s<sub>t</sub>* (the Tic-Tac-Toe board). (Or o<sub>t</sub>, the observable part of the state, in the event the state is not fully observed, and there is some hidden state.)\n",
    "\n",
    "2) From the set of available actions (the open squares), the agent takes *action* *a<sub>t</sub>* (the best move). \n",
    "\n",
    "3) The environment updates at the next *timestep* *t+1* to a new state *s<sub>t+1</sub>*. In Tic-Tac-Toe this is the board resulting from the opponent's move. In a complex environment like a car on a road, the new state may be partly determined by the agent's actions (you turned left and accelerated) and partly by visible or hidden complexities in the environment (a dog runs into the road). And the new state may not be deterministic, it may be stochastic, where some things occur randomly with probabilities dependent on the visible and hidden state and the actions of the agent.\n",
    "\n",
    "4) The environment generates a *reward*. In Tic-Tac-Toe you get a reward when you win, lose, or draw. In Space Invaders, you win points at various times when you hit different targets. When training a self-driving car, machine learning engineers design rewards for staying on the road, getting to the destination, including negative rewards for e.g. collisions.\n",
    "\n",
    "The technical name for this environment is a [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process) (MDP). Reinforcement learning always has states, actions, transitions between states, rewards, and an agent that chooses actions, using a cycle of observing the state, acting, getting a reward and repeating with a new state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Variations\n",
    "\n",
    "The agent always has a *policy function*, which chooses the best action based on the environment state. It *may* have the following components:\n",
    "\n",
    "- *Model* - An internal representation of the environment. Our agent has a model of the board, and it knows some state-action pairs result in the same state as other state-action pairs. Therefore it can be considered *model-based*. A fully model-based algorithm explcitly models the full MDP with all transition probabilities, which this tic-tac-toe algorithm doesn't do. Other agents may be *model-free*. They choose actions without explicitly storing an internal model of the state or modeling state transitions. The understanding of the environment is implicit in the policy function \n",
    "\n",
    "- (footnote. for instance if instead of a table with all possible Tic-Tac-Toe boards we used a table mapping (board, action) pairs to values. Then we wouldn't be modeling internally what happens after a move, i.e. several (board, action) pairs arrive at the same board. We would just evaluate state, action pairs directly without an internal model. That would work too. It would take longer to train since it would be a bigger table.).\n",
    "\n",
    "- *State value function* - A way to score a state (our big table mapping boards to values).\n",
    "\n",
    "- *State-action value function* - A way to score the value of an action in a given state, i.e. a state-action pair, commonly termed a *Q-value function*.\n",
    "\n",
    "Just as there are many algorithms for regression or classification, there are many reinforcement learning architectures. It's a fast-moving field with new approaches emerging constantly. Based on which components a reinforcement learning algorithm uses to generate the workflow shown in Figure 1, it can be characterized as belonging to different flavors of reinforcement learning.\n",
    "\n",
    "![taxonomy](RL3.png \"Figure 2\")\n",
    "\n",
    "All reinforcement learning variations learn using a similar workflow:\n",
    "\n",
    "1) Initialize the algorithm with a naive, possibly random policy.\n",
    "\n",
    "2) Using the policy, take actions, observe states before and after actions, experience rewards.\n",
    "\n",
    "3) Fit a model which improves the policy.\n",
    "\n",
    "4) Go to 2) and iterate, collecting more experience with the improved policy, and continuing to improve it.\n",
    "\n",
    "(find/make a flowchart)\n",
    "\n",
    "As we continue to iterate, we improve the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning In Context\n",
    "\n",
    "In a [previous post](https://alphaarchitect.com/2017/09/27/machine-learning-investors-primer/) we discussed the difference between paradigms of machine learning:\n",
    "\n",
    "*Supervised learning:* Any algorithm that predicts labeled data. Regression predicts a continuous response variable (next quarter's real GDP growth, next month's stock return).  Classification predicts a categorical response variable (recession or recovery, next month's return quintile). \n",
    "\n",
    "*Unsupervised learning:* Any algorithm that summarizes or learns about unlabeled data, such as clustering or dimensionality reduction. \n",
    "\n",
    "Every data set is either labeled or unlabeled, so between supervised and unsupervised, that must cover everything, right? It's like those two books, [What They Teach You At Harvard Business School](https://www.amazon.com/What-Teach-Harvard-Business-School/dp/0141037865) and [What They Don't Teach You At Harvard Business School](https://www.amazon.com/What-Teach-Harvard-Business-School/dp/0553345834). Between the two of them, they must cover all human knowledge, right?\n",
    "\n",
    "Nevertheless reinforcement learning is considered the third major machine learning paradigm. Consider the tic-tac-toe robot:\n",
    "\n",
    "- The agent doesn't have fixed training data, it discovers data via an unsupervised process and learns a policy.\n",
    "\n",
    "- The rewards can be viewed as labels generated by a supervisor. But rewards aren't always directly related to one specific prediction or action. If the agent shoots a target in Space Invaders, it has to figure out which action or sequence of actions possibly many timesteps earlier contributed to the reward (the *credit assigment* problem). \n",
    "\n",
    "- The agent's interactions with the environment *shape* that environment, help determine what data the learning algorithm subsequently encounters, and generate a *feedback loop*. A Space Invaders agent changes the world by shooting targets; a self-driving car doesn't modify the road, but its presence and behavior modify how other vehicles behave, and what environment the algorithm encounters.\n",
    "\n",
    "- In supervised learning, the algorithm optimizes model parameters over training data to minimize a loss function, like mean squared error or cross-entropy. In reinforcement learning, the algorithm optimizes model parameters over the state space it encounters, to maximize the expected reward generated by the Markov Decision Process (MDP) over time.\n",
    "\n",
    "Reinforcement learning could be viewed as meta-supervised-learning. It's the application of supervised machine learning to [*optimal control*](https://en.wikipedia.org/wiki/Optimal_control). We apply supervised prediction methods such as classification and regression. But we use them to predict the best action to take within the *action space*. We move beyond prediction to control. We extend supervised methods to choose actions and learn behavior policies to maximize reward in a complex dynamic environment.\n",
    "\n",
    "Many disciplines have encountered problems like these and developed models and engineering methodologies to adderss them: \n",
    "\n",
    "- Business/Operations Research: Dynamic pricing of airline seats or other products to maximize profits under changing inventory, production, demand conditions.\n",
    "- Economics: Optimal Fed interest rate policy to maintain full employment and low inflation in a dynamic economy.\n",
    "- Engineering: Auto-pilots, spacecraft navigation, robots and industrial automation.\n",
    "- Psychology: Stimulus-response, positive and negative reinforcement.\n",
    "- Neuroscience: The brain's chemical reward loop, how children learn to walk and talk, or catch a ball.\n",
    "- Mathematics: Control theory, game theory, optimization.\n",
    "\n",
    "![connections](RL2.png \"Figure 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning\n",
    "\n",
    "How do we get from our simple Tic-Tac-Toe algorithm to an algorithm that can drive a car or trade a stock?\n",
    "\n",
    "We can view our table lookup as a *linear value function approximator*. If we represent  our board as a one-hot feature vector based on which known board it represents, and view the lookup table as a vector, then if we dot-multiply the one-hot feature vector by the lookup table values, we get a linear value function to choose the next move.\n",
    "\n",
    "Our linear value function approximator takes a board, represents it as a feature vector and outputs a linear function of that feature vector, the value for that board. We can swap that linear function for a nonlinear function, such as neural network. When we do that, we get our first, very crude, deep reinforcement learning algorithm.\n",
    "\n",
    "Our new deep Q-learning algorithm is:\n",
    "\n",
    "1) Initialize our neural network to random weights\n",
    "\n",
    "2) Play a game with our opponent\n",
    "\n",
    "3) At the end of the game, append each board we encountered into a nx9 data array (our predictors are the state of each square) associated with the outcome of the game (our response)\n",
    "\n",
    "4) Fit the neural network to the predictors and responses we've seen (run one or more iterations of stochastic gradient descent)\n",
    "\n",
    "5) Go to 2) and gather more data.\n",
    "\n",
    "This will work, although it takes a long time to train and makes our initial brute force method even more inefficient. (see code). \n",
    "\n",
    "But in a nutshell, that is how a self-driving car could work. \n",
    "\n",
    "- The state is represented by a giant array of inputs from all the onboard cameras and sensors.\n",
    "\n",
    "- The actions are: turn the steering wheel, accelerate, and brake.\n",
    "\n",
    "- Positive rewards come from staying on the road and arriving safely at the destination, and negative rewards from breaking traffic laws or colliding.\n",
    "\n",
    "- The real world provides the state transitions.\n",
    "\n",
    "- And we train a complex neural network to do all the right things involved in detecting and interpreting all the objects in the environment and navigating from point A to point B.\n",
    "\n",
    "Table lookup cannot scale to high dimensional or continuous action or state spaces. And a linear function approximator can't learn nonlinear behavior. With deep neural networks, reinforcement learning algorithms can learn complex emergent behavior. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning for Trading\n",
    "\n",
    "In a trading context, reinforcement learning allows us to use a market signal to create a profitable trading strategy. \n",
    "- The signal can be regression, predicting a continuous variable (link) or classification, predicting a discrete variable e.g. outperform/underperform (binary classification) or quintiles (multinomial classification) (link). \n",
    "- The reward can be raw return or risk-adjusted return (Sharpe). \n",
    "- Reinforcement learning allows you to take a signal and find the optimal policy (trading strategy) to maximize the reward (return or risk-adjusted return).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple example showing how one might trade using reinforcement learning. This approach is inspired by the paper [\"Machine Learning For Trading\" by Gordon Ritter](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3015609).\n",
    "\n",
    "We are going to use simple simulated market data as a stepping stone to more complex trading problem. Let's create a market price time series as a simple sine wave.\n",
    "\n",
    "![Simple harmonic motion 1](StocksSHM1.png \"SHM 1\")\n",
    "\n",
    "- Initially we set the price at 102 and price momentum at 0.\n",
    "- Set 100 as the price 'anchor'. Each timestep, the price accelerates toward 100 by an amount proportional to the distance from 100. If the price is 102:\n",
    "    - the distance from 100 is 2\n",
    "    - the new price momentum is old momentum - 2 * k\n",
    "    - the new price is old price + momentum\n",
    "    \n",
    "This is simple harmonic motion, it describes the oscillation of a spring ([Hooke's law](https://en.wikipedia.org/wiki/Hooke%27s_law), a pendulum under small oscillations, and many other systems with periodic cycles.\n",
    "\n",
    "We can view this as an extremely simplified value/momentum model. 100 is the intrinsic value the stock tends toward. The farther away from intrinsic value, the stronger the acceleration back toward intrinsic value. And momentum means that if the stock is trending up or down, the trend takes time to be reversed.\n",
    "\n",
    "To trade this stock, we use the REINFORCE algorithm, which is a Monte Carlo policy-based method. (We could also use Q-learning, but policy gradient works better.)\n",
    "\n",
    "We will simulate many episodes of 1000 training days, observe the outcomes, and train our policy after each episode.\n",
    "\n",
    "1) Initialize a neural network to choose actions based on the state.\n",
    "  - 2 hidden layers of 16 units\n",
    "  - 32 inputs: the last 16 market values (as deviations from 100 or intrinsic value), and the last 16 daily changes. For our simple harmonic motion with no noise, one input of the last change should be sufficient. But let's use a model we can apply to a more complex example.\n",
    "  - 3 outputs of the probabilities of 0, 1, or 2 for short, flat, long respectively (softmax activation)\n",
    "  - Reward: we buy 1 share based on the model output\n",
    "    - When we choose 2 (long), the next reward is the change in price at the next timestep\n",
    "    - When we choose 1 (flat), the next reward is 0\n",
    "    - When we choose 0 (short), the next reward is the opposite of the change in price at the next timestep\n",
    "  - Choose the initial neural network θ values at random\n",
    "  \n",
    "2) Generate one episode trajectory using the current policy. At each timestep input the current state to the neural network and generate probabilities for short/flat/long. Sample from this generated probability distribution and take the sampled action. Store all the observed states, actions taken, and rewards.\n",
    "\n",
    "3) At the end of the trajectory, back up and compute the discounted future reward observed at each timestep using the action taken and following the current policy to the end of the episode. We can use a large discount because in our model the action taken only impacts the next trading day. In a more complex environment where the current action can impact rewards far in the future, you want to take those rewards into account, and you would use a smaller discount. \n",
    "\n",
    "4) Standardize the returns (discounted future rewards) by subtracting the mean and dividing by standard deviation.\n",
    "\n",
    "5) For each action taken\n",
    "  - Compute the gradient vector of that action probability with respect to the policy thetas (neural network parameters)\n",
    "  - Compute gradient of average return over all observations w.r.t theta: expected value of gradient * return\n",
    "  - Update each theta by its gradient w.r.t. average return times a learning rate. This will update the policy so that\n",
    "    - actions with above-average rewards become more probable\n",
    "    - actions with below-average rewards become less probable\n",
    "\n",
    "6) return to 2) and iterate until the policy is good.\n",
    "\n",
    "Here is a chart of total reward as we train over 1000 episodes. \n",
    "\n",
    "![Simple harmonic motion 2](StocksSHM2.png \"SHM 2\")\n",
    "\n",
    "Finally, here is one sample episode with color coding by short/flat/long and reward over the course of the episode.\n",
    "\n",
    "![Simple harmonic motion 3](StocksSHM3.png \"SHM 2\")\n",
    "\n",
    "\n",
    "It's not perfect, there are a couple of days where it strays from the ideal policy, but it's pretty good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complex example, we take the simple harmonic motion dynamics and add noise + damping.\n",
    "\n",
    "![Simple harmonic motion + noise](SHMplus1.png \"SHM plus noise 1\")\n",
    "\n",
    "![Simple harmonic motion + noise training](SHMplus2.png \"SHM plus noise 2\")\n",
    "\n",
    "![Simple harmonic motion + noise outcome](SHMplus3.png \"SHM plus noise 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possibly\n",
    "- Ornstein-Uhlenbeck process, random walk plus mean reversion, whcih was used in Ritter paper\n",
    "- historical data, maybe with LSTM / attention transformer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(make a table of various methods)\n",
    "\n",
    "- Q learning, doesn't do very well, model-free, value-based, off policy, temporal difference learning\n",
    "- REINFORCE, or policy gradient, model-free, policy-based, on-policy, monte carlo, does better\n",
    "- REINFORCE With Baseline, Actor-Critic: model-free, policy- and value-based, on-policy, Monte Carlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "\n",
    "Similar paradigm to Andrew Lo's adaptive markets https://alo.mit.edu/book/adaptive-markets/\n",
    "\n",
    "bridges gap between prediction and trading. sometimes you find a very small predictive R-squared can lead to large gains. hypothetically, suppose you have a stock market that yields a 5% return. and the top 1% of days are up 5% and the top bottom percent of days are down 1%. Suppose your prediction model gets all of those extreme days right. With 51% accuracy you doubled your return. Suppose you get 51% evenly distributed, you get a tiny increment.\n",
    "\n",
    "how different from backtesting? can use much more complex models where there are too many parameter combinations to backtest.\n",
    "\n",
    "do a lot of prediction, attempt to use it as input to a strategy. we find very often in deep learning that when we train complex models with many different moving parts end to end, this results in interesting emergent behavior. makes sense to try to train for prediction and control end to end.\n",
    "\n",
    "algorithms own very short term hf trading. with algorithms like RL they can move up the timescale.\n",
    "\n",
    "JPMorgan and others reportedly using RL to trade in the real world https://arxiv.org/pdf/1802.03042.pdf https://informaconnect.com/the-latest-in-loxm-and-why-we-shouldnt-be-using-single-stock-algos/ \n",
    "\n",
    "Deep reinforcement learning is hard. We're not yet at the point where you can point a universal agent at a problem and expect a quick solution. Deep rl is very data-hungry or sample-inefficient. people are working on it https://www.microsoft.com/en-us/research/blog/provably-efficient-reinforcement-learning-with-rich-observations/?ocid=msr_blog_provably_icml_hero It's not something you can do with a few years of quarterly financials.\n",
    "\n",
    "You also need a lot of experimentation and tuning and testing in an adversarial environment. \n",
    "\n",
    "https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "\n",
    "rl is not infallible or perfectly stable. reinforcement learning algorithms that constantly train on recent experience forget things they learned at the beginning of training, and suddenly start to underperform. you need good tradeoff between exploitation and exploration that covers the full state/action space. Commercial pilots can go through thousands of flights without encountering a real in-flight emergency. In order to handle those emergencies, they need recurrent training where all kinds of failures are simulated. \n",
    "\n",
    "Algorithms can be exploited. Self-driving vehicles on the streets of New York or New Delhi seem unlikely in the near future, without changes like protected lanes for self-driving vehicles and strong enforcement. If pedestrians know that the other driver is always going to stop for them no matter what, they will learn to just cross at the red light, never mind traffic. They can even wear a stop sign on a T-shirt. It's not a matter of how good the self-driving technology is, it's a matter of game theory. Knowing that the other driver is a fallible human who at best may be angry and honk and give you the finger, and at worst may be on a cell phone and not even see you tends to concentrate the mind. \n",
    "\n",
    "In our tic-tac-toe example, the reinforcement learning algorithm adapts to the player. The player who understands the algorithm can exploit this, taking a loss to teach the algorithm a bad example the human player can exploit many times before the algorithm catches up. It's like a poker player showing a bluff to get other players to loosen up, or the nut flush to get a little more respect for bluffs. \n",
    "\n",
    "In a worst case, an adversarial sticker can make image recognition think a banana is a toaster or an adversarial temporary tattoo can defeat face recognition \n",
    "https://medium.com/deep-learning-cafe/neural-networks-easily-fooled-e19bf575b527\n",
    "https://cvdazzle.com/\n",
    "\n",
    "This is a problem for trading with reinforcement learning. if a market maker algorithm trades on patterns, adversarial algorithms can learn to exploit it. you can't \n",
    "\n",
    "nevertheless anecdotally poker bots taking over,\n",
    "\n",
    "there's a need to combine brute force RL with some reasoning about the world or the game, which AlphaGo and AlphaZero are able to do to some extent.\n",
    "\n",
    "\n",
    "like the man in the moliere play who discovers he's been speaking in prose, you've been doing optimal control your whole life without realizing it.\n",
    "\n",
    "Further reading\n",
    "\n",
    "Courses\n",
    "- [UCL course by David Silver (videos and lecture notes)](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "- [Stanford course](http://web.stanford.edu/class/cs234/schedule.html)\n",
    "- [Berkeley course](http://rail.eecs.berkeley.edu/deeprlcourse/)\n",
    "\n",
    "Books\n",
    "- [Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- https://www.amazon.com/Deep-Reinforcement-Learning-Python-Hands/dp/0135172381\n",
    "- [Algorithms for Reinforcement Learning, Csaba Szepesvári](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n",
    "- http://www.deeplearningbook.org/\n",
    "- https://www.amazon.com/Artificial-Intelligence-Modern-Approach-4th/dp/0134610997/ref=dp_ob_title_bk\n",
    "\n",
    "- http://karpathy.github.io/2016/05/31/rl/\n",
    "- https://arxiv.org/pdf/1802.03042.pdf\n",
    "\n",
    "- https://spinningup.openai.com/en/latest/spinningup/keypapers.html\n",
    "- https://www.econstor.eu/bitstream/10419/183139/1/1032172355.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
