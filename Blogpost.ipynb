{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Properly used, positive reinforcement is extremely powerful. - [B. F. Skinner](https://www.brainyquote.com/authors/b-f-skinner-quotes)*\n",
    "\n",
    "*once one of my pups found half a roast chicken in the corner of a parking lot and we had to visit that exact same corner every day for about fifty years because for dogs hope springs eternal when it comes to half a roast chicken - [darth](https://twitter.com/darth/status/1057075608063139840)* (possibly embed tweet)\n",
    "\n",
    "Tic-Tac-Toe is a simple game. If both sides play perfectly, neither can win. But if one plays imperfectly the other can exploit the flaws in the other's strategy. \n",
    "\n",
    "Does that sound a little like trading?\n",
    "\n",
    "Here's an algorithm that will learn an exploitive Tic-Tac-Toe strategy, and if its opponent changes, over time it will adapt:\n",
    "\n",
    "1) Make a big table of all possible Tic-Tac-Toe boards. \n",
    "\n",
    "2) Initialize the table to assign a value of 0 to each board, 1.0 where X has won, -1.0 where O has won.\n",
    "\n",
    "3) Play with your opponent. At each move, pick the best available move in your table, or if several are tied for best, pick one at random. Occasionally pick a move at random just to make sure you explore the whole state space, and keep your opponent on their toes. \n",
    "\n",
    "4) After every game, back up through all the moves you played. If you won, update your value table as follows:\n",
    "\t- When X wins, update each board's value part of the way to 1. \n",
    "\t- When O wins, update part of the way to -1.  \n",
    "\t- When they tie, update part of the way to 0 \n",
    "\n",
    "This is a very crude brute-force algorithm. It knows almost nothing about the game of Tic-Tac-Toe. It can't reason about the game. It can't generalize to boards it hasn't seen (footnote: even if they are isomorphic to boards it has seen. When you think about it, there are only 3 starting moves, board center, corner, center side. Flipping or rotating the board shouldn't change the value of a position or how to play it.) . It doesn't find the globally optimal strategy. \n",
    "\n",
    "But over time, this algorithm will learn, it will exploit flaws in its opponent's strategy, and if the opponent changes tactics, over time it will adapt.\n",
    "\n",
    "This is *reinforcement learning*. (link to code)\n",
    "\n",
    "More sophisticated reinforcement algorithms enable [robots to walk on four or two legs](https://www.youtube.com/watch?v=xXrDnq1RPzQ), [driverless cars to drive](https://www.youtube.com/watch?v=eRwTbRtnT1I), computers to play [Atari](https://deepsense.ai/playing-atari-with-deep-reinforcement-learning-deepsense-ais-approach/) and [poker](https://www.engadget.com/2017/02/10/libratus-ai-poker-winner/?guccounter=1) and [Go](https://deepmind.com/blog/article/alphago-zero-starting-scratch) in some cases better than humans. \n",
    "\n",
    "In this post we'll extend the Tic-Tac-Toe example to more complex deep reinforcement learning, and try to build a reinforcement learning trading robot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Concepts\n",
    "\n",
    "How does reinforcement learning work?\n",
    "\n",
    "![agent-environment](RL1.png \"Figure 1\")\n",
    "\n",
    "all these figures are from Silver - http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "\n",
    "1) At time t, The *agent* observes the environment *state* s sub t (the Tic-Tac-Toe board)\n",
    "\n",
    "2) From the set of available actions (the open squares), the agent takes *action* a sub t (the move that results in the best probability of winning) \n",
    "\n",
    "3) The environment updates at the next *timestep* t+1 to a new state s sub t+1. In Tic-Tac-Toe this is the agent's board target. But in a complex environment like a car on a road, the new state may be partly determined by the agent's actions (you turned left and accelerated) and partly by visible or hidden complexities in the environment (a dog runs into the road). And the new state may not be deterministic, it may be stochastic, where some things occur randomly with probabilities dependent on the visible and hidden state and the actions of the agent.\n",
    "\n",
    "4) The environment generates a reward. In Tic-Tac-Toe this happens when you win, lose, or draw. In Space Invaders, you get points awarded at various times when you hit different targets. When training a self-driving car, machine learning engineers have to design rewards for e.g. staying on the road, including negative rewards for e.g. colliding with something.\n",
    "\n",
    "The technical name for this environment is a [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process). Reinforcement learning always has states, actions, transitions between states, rewards, and an agent that chooses actions, and this cycle of observing the state, acting, getting a reward and repeating with the new state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Variations\n",
    "\n",
    "The agent always has a *policy function*, which chooses the best action based on the environment state. It *may* have the following components:\n",
    "\n",
    "- *Model* - An internal representation of the environment. Our algorithm stores the board. It knows some state-action pairs result in same state as other state-action pairs, so it's *model-based*.\n",
    "\n",
    "- *State value function* - the ability to score a state (our V table)\n",
    "\n",
    "- *State-action value function* - the ability to score the value of an action in a given state, i.e. a state-action pair, commonly termed the *Q-value function*\n",
    "\n",
    "Based on which components a reinforcement algorithm uses to generate the workflow shown in figure, it can be characterized as belonging to different flavors.\n",
    "\n",
    "![taxonomy](RL3.png \"Figure 2\")\n",
    "\n",
    "But all reinforcement learning learns by a similar process:\n",
    "\n",
    "1) Initialize the algorithm with a naive, possibly random policy \n",
    "\n",
    "2) Observe states, take actions, collect data, experience rewards \n",
    "\n",
    "3) Fit a model, which improves the policy\n",
    "\n",
    "4) Go to 2) and repeat, to collect more experience with our improved policy, and keep improving it.\n",
    "\n",
    "As we circle the flowchart, we improve the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning In Context\n",
    "\n",
    "In a [previous post](https://alphaarchitect.com/2017/09/27/machine-learning-investors-primer/) we talked about types of machine learning:\n",
    "\n",
    "Supervised learning: Any algorithm that predicts labeled data. Regression predicts a continuous response variable.  Classification predicts a discrete response variable. \n",
    "\n",
    "Unsupervised learning: Any algorithm that summarizes or learns something about unlabled data, such as clustering, dimensionality reduction. \n",
    "\n",
    "It's like those two books, [What They Teach You At Harvard Business School](https://www.amazon.com/What-Teach-Harvard-Business-School/dp/0141037865) and [What They Don't Teach You At Harvard Business School](https://www.amazon.com/What-Teach-Harvard-Business-School/dp/0553345834). Between the two of them, they must cover everything, right? Nevertheless reinforcement learning is considered the third major machine learning paradigm. \n",
    "\n",
    "- The agent doesn't have labeled data, it has to proactively go out and discover data and figure out what to do.\n",
    "\n",
    "- The rewards can be viewed as labels generated by a supervisor. But rewards aren't directly related to any specific prediction or action. If the agent shoots a target in Space Invaders, it has to figure out which action or actions possibly several timesteps earlier contributed to the reward. \n",
    "\n",
    "- The agent's interactions with the environment *shape* that environment and generate a feedback loop. (A Space Invaders agent changes the world by shooting targets; a self-driving car doesn't modify the road, but its presence and behavior modifies how other vehicles behave, and what environment the algorithm encounters.)\n",
    "\n",
    "- In supervised learning, the algorithm minimizes an error, like mean-square error or cross-entropy, by optimizing model parameters over your training data. In reinforcement learning, the algorithm maximizes the expected cumulative reward generated by the Markov Decision Process (MDP) over time, by searching the state space and optimizing model parameters.\n",
    "\n",
    "I tend to view reinforcement learning as meta-supervised-learning. It's the application of machine algorithms to prediction in a complex dynamic environment, and then *extending* those algorithms beyond prediction to *control*, using the predictions to design complex behavior to maximize reward.\n",
    "\n",
    "Reinforcement learning is something we do under different names in many problem-solving contexts across many disciplines. \n",
    "\n",
    "![taxonomy](RL2.png \"Figure 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning\n",
    "\n",
    "How do we get from our simple Tic-Tac-Toe algorithm to an algorithm that can drive a car?\n",
    "\n",
    "We can view our table lookup-based algorithm as a model with a *linear value function approximator*. If we represent  our board as a one-hot vector based on which known board it represents, and view the lookup table as a vector, if multiply the one-hot feature vector by the values, we get a linear value function.\n",
    "\n",
    "Our linear function approximator, takes a board, converts it to a feature vector and outputs a linear function of that feature vector to generate a value for that board. We can swap that linear function for a nonlinear function, such as neural network. When we do that, we get a first, very crude, deep reinforcement learning network.\n",
    "\n",
    "Our new algorithm is:\n",
    "\n",
    "1) Initialize our neural network to random weights\n",
    "\n",
    "2) Play a game with our opponent\n",
    "\n",
    "3) At the end of the game, put each board we encountered into a 1x9 array (our predictors) associated with the outcome of the game (our response)\n",
    "\n",
    "4) Fit the neural network to all the predictors and responses we've seen (run one or more iterations of stochastic gradient descent)\n",
    "\n",
    "5) Go to 2) and gather more data.\n",
    "\n",
    "This will work, although it takes a long time to train and just makes our initial brute force method even more inefficient. (see code). \n",
    "\n",
    "But in a nutshell, that is how a self-driving car works. The input is a giant multi-dimensional array of all the onboard cameras and sensors, the outputs are actions like turn the steering wheel, accelerate, and brake, and we  train a complex neural network to do all the right things involved in detecting and interpreting all the objects in the environment and navigating from point A to point B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do a trading example.\n",
    "\n",
    "- inspired by Gordon Ritter paper https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3015609\n",
    "\n",
    "- let's create a stocks. give them each a random walk, so they are flat. now make it springy. random walk plus momentum, plus attraction back to the mean, , plus trend.\n",
    "\n",
    "- let reinforcement learning trade it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "\n",
    "Similar paradigm to Andrew Lo's adaptive markets https://alo.mit.edu/book/adaptive-markets/\n",
    "\n",
    "anecdotally poker bots taking over, algorithms own very short term hf trading. algorithms like RL they can move up the timescale.\n",
    "\n",
    "there's a need to combine the brute force RL stuff with some reasoning about the world or the game, which AlphaGo and AlphaZero seem to do to some modest extent.\n",
    "\n",
    "further reading\n",
    "\n",
    "- UCL course by David Silver http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "- stanford course  http://web.stanford.edu/class/cs234/schedule.html\n",
    "- berkeley course http://rail.eecs.berkeley.edu/deeprlcourse/\n",
    "- sutton book http://incompleteideas.net/book/the-book-2nd.html\n",
    "- https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
